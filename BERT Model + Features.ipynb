{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40756fa7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1471d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67ad8f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>pos_ratios</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>386</td>\n",
       "      <td>1875</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.031088082901554404, 'JJ': 0.05181347...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>0.385613</td>\n",
       "      <td>dear local newspaper think effect computer peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>2288</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.03879310344827586, ',': 0.0258620689...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.613167</td>\n",
       "      <td>dear believe using computer benefit u many way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>1541</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.04153354632587859, ',': 0.0287539936...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340393</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>dear people use computer everyone agrees benef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611</td>\n",
       "      <td>3165</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.11620294599018004, ',': 0.0212765957...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.441795</td>\n",
       "      <td>dear local newspaper found many expert say com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>517</td>\n",
       "      <td>2569</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.017408123791102514, ',': 0.025145067...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>dear know computer positive effect people comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  word_len  chars_len  \\\n",
       "0             NaN             NaN            NaN  ...       386       1875   \n",
       "1             NaN             NaN            NaN  ...       464       2288   \n",
       "2             NaN             NaN            NaN  ...       313       1541   \n",
       "3             NaN             NaN            NaN  ...       611       3165   \n",
       "4             NaN             NaN            NaN  ...       517       2569   \n",
       "\n",
       "   avg_word_length  avg_sentence_length  \\\n",
       "0         3.984456                  1.0   \n",
       "1         4.030172                  1.0   \n",
       "2         4.035144                  1.0   \n",
       "3         4.328969                  1.0   \n",
       "4         4.071567                  1.0   \n",
       "\n",
       "                                          pos_ratios  num_sentences  \\\n",
       "0  {'NNP': 0.031088082901554404, 'JJ': 0.05181347...             16   \n",
       "1  {'NNP': 0.03879310344827586, ',': 0.0258620689...             20   \n",
       "2  {'NNP': 0.04153354632587859, ',': 0.0287539936...             14   \n",
       "3  {'NNP': 0.11620294599018004, ',': 0.0212765957...             27   \n",
       "4  {'NNP': 0.017408123791102514, ',': 0.025145067...             30   \n",
       "\n",
       "   num_paragraphs  sentiment_polariy  sentiment_subjectivity  \\\n",
       "0               1           0.310471                0.385613   \n",
       "1               1           0.274000                0.613167   \n",
       "2               1           0.340393                0.498657   \n",
       "3               1           0.266828                0.441795   \n",
       "4               1           0.199684                0.485814   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear local newspaper think effect computer peo...  \n",
       "1  dear believe using computer benefit u many way...  \n",
       "2  dear people use computer everyone agrees benef...  \n",
       "3  dear local newspaper found many expert say com...  \n",
       "4  dear know computer positive effect people comp...  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "725b81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdd0fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay', 'rater1_domain1', 'rater2_domain1']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b76790d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the precision score between the true and predicted values\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calc_recall(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the recall score between the true and predicted values\n",
    "    \"\"\"\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calc_f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the f1-score between the true and predicted values\n",
    "    \"\"\"\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calc_cohen_kappa_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the cohen kappa score between the true and predicted values\n",
    "    \"\"\"\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calc_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy score between the true and predicted values\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23173639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_function(y_actual, y_predictions, print_metrics = False):\n",
    "    \n",
    "    accuracy = calc_accuracy(y_actual, y_predictions)\n",
    "    precision = calc_precision(y_actual, y_predictions)\n",
    "    recall = calc_recall(y_actual, y_predictions)\n",
    "    f1 = calc_f1_score(y_actual, y_predictions)\n",
    "    kappa_score = calc_cohen_kappa_score(y_actual, y_predictions)\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1-Score:\", f1)\n",
    "        print(\"Cohen Kappa Score:\", kappa_score)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "173c38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a75f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_classifiers(classifier_name = \"logistic_regression\"):\n",
    "    \"\"\"\n",
    "    Takes a regressor as input and returns a corresponding classifier object\n",
    "    \"\"\"\n",
    "    \n",
    "    if classifier_name == 'logistic_regression':\n",
    "        return LogisticRegression()\n",
    "    elif classifier_name == 'decision_tree_classifier':\n",
    "        return DecisionTreeClassifier()\n",
    "    elif classifier_name == 'random_forest_classifier':\n",
    "        return RandomForestClassifier()\n",
    "    elif classifier_name == 'gradient_boosting_classifier':\n",
    "        return GradientBoostingClassifier()\n",
    "    elif classifier_name == 'adaboost_classifier':\n",
    "        return AdaBoostClassifier()\n",
    "    elif classifier_name == 'k_neighbors_classifier':\n",
    "        return KNeighborsClassifier()\n",
    "    elif classifier_name == 'support_vector_classifier':\n",
    "        return SVC()\n",
    "    elif classifier_name == 'xgboost_classifier':\n",
    "        return XGBClassifier()\n",
    "    elif classifier_name == 'gaussian_naive_bayes_classifier':\n",
    "        return GaussianNB()\n",
    "    else:\n",
    "        raise ValueError(f\"Classifier {classifier_name} not supported for this problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2344ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 1]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d2dd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrector(tokens):\n",
    "    spell_checker = SpellChecker()\n",
    "    correct_tokens = []\n",
    "    for token in tqdm(tokens):\n",
    "        if spell_checker.correction(token.lower()):\n",
    "            correct_tokens.append(spell_checker.correction(token.lower()))\n",
    "        else:\n",
    "            correct_tokens.append(token.lower())\n",
    "    \n",
    "    return ' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "093a9056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>1</td>\n",
       "      <td>431</td>\n",
       "      <td>1993</td>\n",
       "      <td>3.781903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096094</td>\n",
       "      <td>0.511334</td>\n",
       "      <td>dear local people using computer year good hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>1498</td>\n",
       "      <td>4.283737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066142</td>\n",
       "      <td>0.514918</td>\n",
       "      <td>dear newspaper believe computer positive affec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>2724</td>\n",
       "      <td>4.037770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262030</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>people agree computer make life le complicated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>2259</td>\n",
       "      <td>3.837838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.099660</td>\n",
       "      <td>0.426994</td>\n",
       "      <td>dear world changed much better technology beco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>1</td>\n",
       "      <td>488</td>\n",
       "      <td>2369</td>\n",
       "      <td>3.954918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053060</td>\n",
       "      <td>0.429295</td>\n",
       "      <td>technology growing changing rapidly look apple...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_set  word_len  chars_len  avg_word_length  avg_sentence_length  \\\n",
       "1346          1       431       1993         3.781903                  1.0   \n",
       "1349          1       289       1498         4.283737                  1.0   \n",
       "7             1       556       2724         4.037770                  1.0   \n",
       "1251          1       481       2259         3.837838                  1.0   \n",
       "661           1       488       2369         3.954918                  1.0   \n",
       "\n",
       "      num_sentences  num_paragraphs  sentiment_polariy  \\\n",
       "1346             37               1           0.096094   \n",
       "1349             15               1           0.066142   \n",
       "7                39               1           0.262030   \n",
       "1251             31               1           0.099660   \n",
       "661              26               1           0.053060   \n",
       "\n",
       "      sentiment_subjectivity  \\\n",
       "1346                0.511334   \n",
       "1349                0.514918   \n",
       "7                   0.574500   \n",
       "1251                0.426994   \n",
       "661                 0.429295   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "1346  dear local people using computer year good hea...  \n",
       "1349  dear newspaper believe computer positive affec...  \n",
       "7     people agree computer make life le complicated...  \n",
       "1251  dear world changed much better technology beco...  \n",
       "661   technology growing changing rapidly look apple...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eca450",
   "metadata": {},
   "source": [
    "### BERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c4097a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77f9db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:16<00:00,  5.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:04<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce28fa",
   "metadata": {},
   "source": [
    "#### Adding new features from feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3be90f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17fdb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_function(input_data, output_data, ml_model = \"logistic_regression\",\n",
    "                             print_results = True, n_folds = 5, return_kappa_scores = True):\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    kappa_score_list = []\n",
    "    \n",
    "    k_fold = KFold(n_splits=n_folds, shuffle=True, random_state=101)\n",
    "    X = pd.DataFrame(input_data.numpy()).copy()\n",
    "    y = output_data.copy()\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(k_fold.split(X, y)):\n",
    "        \n",
    "        train_embeddings, train_y = X.iloc[train_indices], y.iloc[train_indices]\n",
    "        val_embeddings, val_y = X.iloc[val_indices], y.iloc[val_indices]\n",
    "        model = choose_classifiers(ml_model)\n",
    "        model.fit(train_embeddings, train_y)\n",
    "        y_predictions = model.predict(val_embeddings)\n",
    "        accuracy_logistic_reg, precision_logistic_reg, recall_logistic_reg, f1_logistic_reg, kappa_score_logistic_reg = print_metrics_function(val_y, y_predictions)\n",
    "        accuracy_list.append(accuracy_logistic_reg)\n",
    "        precision_list.append(precision_logistic_reg)\n",
    "        recall_list.append(recall_logistic_reg)\n",
    "        f1_list.append(f1_logistic_reg)\n",
    "        kappa_score_list.append(kappa_score_logistic_reg)\n",
    "    \n",
    "    if print_results:\n",
    "        \n",
    "        print(\"Accuracy: {:.4f}\".format(np.max(accuracy_list)))\n",
    "        print(\"Precision: {:.4f}\".format(np.max(precision_list)))\n",
    "        print(\"Recall: {:.4f}\".format(np.max(recall_list)))\n",
    "        print(\"F1 score: {:.4f}\".format(np.max(f1_list)))\n",
    "        print(\"Kappa score: {:.4f}\".format(np.max(kappa_score_list)))\n",
    "        \n",
    "    if return_kappa_scores:\n",
    "        \n",
    "        return round(np.max(kappa_score_list), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "992dfa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5263\n",
      "Precision: 0.4037\n",
      "Recall: 0.4122\n",
      "F1 score: 0.3739\n",
      "Kappa score: 0.7927\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.5368\n",
      "Precision: 0.4617\n",
      "Recall: 0.4008\n",
      "F1 score: 0.3891\n",
      "Kappa score: 0.7731\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5018\n",
      "Precision: 0.1932\n",
      "Recall: 0.2654\n",
      "F1 score: 0.2188\n",
      "Kappa score: 0.6589\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.4790\n",
      "Precision: 0.3512\n",
      "Recall: 0.3299\n",
      "F1 score: 0.2910\n",
      "Kappa score: 0.7312\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5228\n",
      "Precision: 0.1770\n",
      "Recall: 0.1927\n",
      "F1 score: 0.1713\n",
      "Kappa score: 0.6565\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set1 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ab54c",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb0492bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 2]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f43dceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:16<00:00,  5.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:04<00:00,  5.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1efc10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c05c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6806\n",
      "Precision: 0.6100\n",
      "Recall: 0.4372\n",
      "F1 score: 0.4577\n",
      "Kappa score: 0.6767\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6736\n",
      "Precision: 0.6225\n",
      "Recall: 0.4873\n",
      "F1 score: 0.5279\n",
      "Kappa score: 0.6341\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.6701\n",
      "Precision: 0.3333\n",
      "Recall: 0.3530\n",
      "F1 score: 0.3424\n",
      "Kappa score: 0.5789\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.6458\n",
      "Precision: 0.4791\n",
      "Recall: 0.3638\n",
      "F1 score: 0.3803\n",
      "Kappa score: 0.5841\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.6979\n",
      "Precision: 0.4195\n",
      "Recall: 0.3826\n",
      "F1 score: 0.3890\n",
      "Kappa score: 0.6294\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set2 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed2079",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed773749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 3]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4de8bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [00:12<00:00,  7.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:02<00:00, 10.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3abd8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55f518ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6848\n",
      "Precision: 0.7678\n",
      "Recall: 0.5915\n",
      "F1 score: 0.6182\n",
      "Kappa score: 0.6857\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6739\n",
      "Precision: 0.7497\n",
      "Recall: 0.5580\n",
      "F1 score: 0.5871\n",
      "Kappa score: 0.6806\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5870\n",
      "Precision: 0.5448\n",
      "Recall: 0.4801\n",
      "F1 score: 0.4691\n",
      "Kappa score: 0.6449\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.6341\n",
      "Precision: 0.7022\n",
      "Recall: 0.4946\n",
      "F1 score: 0.5127\n",
      "Kappa score: 0.6236\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.6739\n",
      "Precision: 0.5169\n",
      "Recall: 0.5343\n",
      "F1 score: 0.5113\n",
      "Kappa score: 0.6986\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set3 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725c1c9",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c2f451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 4]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c3e350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 89/89 [00:11<00:00,  7.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00,  9.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "728ac2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34d527fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6360\n",
      "Precision: 0.6564\n",
      "Recall: 0.6140\n",
      "F1 score: 0.6275\n",
      "Kappa score: 0.7440\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6360\n",
      "Precision: 0.6535\n",
      "Recall: 0.6161\n",
      "F1 score: 0.6273\n",
      "Kappa score: 0.7382\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5830\n",
      "Precision: 0.5731\n",
      "Recall: 0.5766\n",
      "F1 score: 0.5650\n",
      "Kappa score: 0.7167\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.5830\n",
      "Precision: 0.5866\n",
      "Recall: 0.5840\n",
      "F1 score: 0.5848\n",
      "Kappa score: 0.7233\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.6113\n",
      "Precision: 0.5102\n",
      "Recall: 0.5134\n",
      "F1 score: 0.4809\n",
      "Kappa score: 0.6575\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set4 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdff4e",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b23b57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 5]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e3e002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:13<00:00,  6.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00,  8.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e0b5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0976af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.7197\n",
      "Precision: 0.7630\n",
      "Recall: 0.7175\n",
      "F1 score: 0.6969\n",
      "Kappa score: 0.8240\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6817\n",
      "Precision: 0.5722\n",
      "Recall: 0.5605\n",
      "F1 score: 0.5569\n",
      "Kappa score: 0.8182\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5744\n",
      "Precision: 0.3132\n",
      "Recall: 0.3361\n",
      "F1 score: 0.2976\n",
      "Kappa score: 0.5894\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.6471\n",
      "Precision: 0.5318\n",
      "Recall: 0.5421\n",
      "F1 score: 0.5360\n",
      "Kappa score: 0.7742\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.6955\n",
      "Precision: 0.5774\n",
      "Recall: 0.5601\n",
      "F1 score: 0.5669\n",
      "Kappa score: 0.7950\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set5 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15025161",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8dfb16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 6]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9c4d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:16<00:00,  5.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02467133",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3e2cd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6910\n",
      "Precision: 0.6642\n",
      "Recall: 0.5349\n",
      "F1 score: 0.5534\n",
      "Kappa score: 0.7551\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6424\n",
      "Precision: 0.6944\n",
      "Recall: 0.5355\n",
      "F1 score: 0.5464\n",
      "Kappa score: 0.7430\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5139\n",
      "Precision: 0.4812\n",
      "Recall: 0.4547\n",
      "F1 score: 0.4258\n",
      "Kappa score: 0.6854\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.5764\n",
      "Precision: 0.5358\n",
      "Recall: 0.4971\n",
      "F1 score: 0.4899\n",
      "Kappa score: 0.6841\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.6424\n",
      "Precision: 0.5688\n",
      "Recall: 0.3985\n",
      "F1 score: 0.4066\n",
      "Kappa score: 0.6161\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set6 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4aed3a",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e32874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 7]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "633cd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:15<00:00,  5.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc520228",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "edc4e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.1753\n",
      "Precision: 0.1488\n",
      "Recall: 0.1286\n",
      "F1 score: 0.1116\n",
      "Kappa score: 0.6873\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.1713\n",
      "Precision: 0.1158\n",
      "Recall: 0.1165\n",
      "F1 score: 0.1041\n",
      "Kappa score: 0.6895\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.1873\n",
      "Precision: 0.0544\n",
      "Recall: 0.0883\n",
      "F1 score: 0.0442\n",
      "Kappa score: 0.4780\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.1315\n",
      "Precision: 0.1184\n",
      "Recall: 0.1681\n",
      "F1 score: 0.1295\n",
      "Kappa score: 0.5982\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.1952\n",
      "Precision: 0.0284\n",
      "Recall: 0.0891\n",
      "F1 score: 0.0378\n",
      "Kappa score: 0.4162\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set7 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0a9a2",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51fd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 8]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f42e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:06<00:00,  5.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be0f61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cbd7cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.2241\n",
      "Precision: 0.1046\n",
      "Recall: 0.1048\n",
      "F1 score: 0.0910\n",
      "Kappa score: 0.6475\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.2500\n",
      "Precision: 0.0591\n",
      "Recall: 0.0589\n",
      "F1 score: 0.0376\n",
      "Kappa score: 0.4778\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.2845\n",
      "Precision: 0.0221\n",
      "Recall: 0.0808\n",
      "F1 score: 0.0345\n",
      "Kappa score: 0.5583\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.1983\n",
      "Precision: 0.1076\n",
      "Recall: 0.1197\n",
      "F1 score: 0.0942\n",
      "Kappa score: 0.5377\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.2696\n",
      "Precision: 0.0119\n",
      "Recall: 0.0476\n",
      "F1 score: 0.0188\n",
      "Kappa score: 0.1712\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set8 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b07694b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_qwk = [logistic_regression_qwk_set1, logistic_regression_qwk_set2, logistic_regression_qwk_set3,\n",
    "                           logistic_regression_qwk_set4, logistic_regression_qwk_set5, logistic_regression_qwk_set6,\n",
    "                           logistic_regression_qwk_set7, logistic_regression_qwk_set8]\n",
    "random_forest_classifier_qwk = [random_forest_classifier_qwk_set1, random_forest_classifier_qwk_set2, random_forest_classifier_qwk_set3,\n",
    "                                random_forest_classifier_qwk_set4, random_forest_classifier_qwk_set5, random_forest_classifier_qwk_set6,\n",
    "                                random_forest_classifier_qwk_set7, random_forest_classifier_qwk_set8]\n",
    "adaboost_classifier_qwk = [adaboost_classifier_qwk_set1, adaboost_classifier_qwk_set2, adaboost_classifier_qwk_set3,\n",
    "                           adaboost_classifier_qwk_set4, adaboost_classifier_qwk_set5, adaboost_classifier_qwk_set6,\n",
    "                           adaboost_classifier_qwk_set7, adaboost_classifier_qwk_set8]\n",
    "k_neighbors_classifier_qwk = [k_neighbors_classifier_qwk_set1, k_neighbors_classifier_qwk_set2, k_neighbors_classifier_qwk_set3,\n",
    "                              k_neighbors_classifier_qwk_set4, k_neighbors_classifier_qwk_set5, k_neighbors_classifier_qwk_set6,\n",
    "                              k_neighbors_classifier_qwk_set7, k_neighbors_classifier_qwk_set8]\n",
    "support_vector_classifier_qwk = [support_vector_classifier_qwk_set1, support_vector_classifier_qwk_set2, support_vector_classifier_qwk_set3,\n",
    "                                 support_vector_classifier_qwk_set4, support_vector_classifier_qwk_set5, support_vector_classifier_qwk_set6,\n",
    "                                 support_vector_classifier_qwk_set7, support_vector_classifier_qwk_set8]\n",
    "\n",
    "metrics_list = [logistic_regression_qwk, random_forest_classifier_qwk, adaboost_classifier_qwk,\n",
    "               k_neighbors_classifier_qwk, support_vector_classifier_qwk]\n",
    "\n",
    "results_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "results_df.rename(columns = {0: 'Prompt-1', 1: 'Prompt-2', 2: 'Prompt-3', \n",
    "                            3: 'Prompt-4', 4: 'Prompt-5', 5: 'Prompt-6',\n",
    "                            6: 'Prompt-7', 7: 'Prompt-8'}, inplace = True)\n",
    "\n",
    "results_df.rename(index = {0: 'BERT + LR', 1: 'BERT + RF', \n",
    "                           2: 'BERT + Adaboost', 3: 'BERT + KNN', 4: 'BERT + SVC'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd794951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('Results/BERT + features.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
