{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5815cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1471d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ad8f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>pos_ratios</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>386</td>\n",
       "      <td>1875</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.031088082901554404, 'JJ': 0.05181347...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>0.385613</td>\n",
       "      <td>dear local newspaper think effect computer peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>2288</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.03879310344827586, ',': 0.0258620689...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.613167</td>\n",
       "      <td>dear believe using computer benefit u many way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>1541</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.04153354632587859, ',': 0.0287539936...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340393</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>dear people use computer everyone agrees benef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611</td>\n",
       "      <td>3165</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.11620294599018004, ',': 0.0212765957...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.441795</td>\n",
       "      <td>dear local newspaper found many expert say com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>517</td>\n",
       "      <td>2569</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.017408123791102514, ',': 0.025145067...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>dear know computer positive effect people comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  word_len  chars_len  \\\n",
       "0             NaN             NaN            NaN  ...       386       1875   \n",
       "1             NaN             NaN            NaN  ...       464       2288   \n",
       "2             NaN             NaN            NaN  ...       313       1541   \n",
       "3             NaN             NaN            NaN  ...       611       3165   \n",
       "4             NaN             NaN            NaN  ...       517       2569   \n",
       "\n",
       "   avg_word_length  avg_sentence_length  \\\n",
       "0         3.984456                  1.0   \n",
       "1         4.030172                  1.0   \n",
       "2         4.035144                  1.0   \n",
       "3         4.328969                  1.0   \n",
       "4         4.071567                  1.0   \n",
       "\n",
       "                                          pos_ratios  num_sentences  \\\n",
       "0  {'NNP': 0.031088082901554404, 'JJ': 0.05181347...             16   \n",
       "1  {'NNP': 0.03879310344827586, ',': 0.0258620689...             20   \n",
       "2  {'NNP': 0.04153354632587859, ',': 0.0287539936...             14   \n",
       "3  {'NNP': 0.11620294599018004, ',': 0.0212765957...             27   \n",
       "4  {'NNP': 0.017408123791102514, ',': 0.025145067...             30   \n",
       "\n",
       "   num_paragraphs  sentiment_polariy  sentiment_subjectivity  \\\n",
       "0               1           0.310471                0.385613   \n",
       "1               1           0.274000                0.613167   \n",
       "2               1           0.340393                0.498657   \n",
       "3               1           0.266828                0.441795   \n",
       "4               1           0.199684                0.485814   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear local newspaper think effect computer peo...  \n",
       "1  dear believe using computer benefit u many way...  \n",
       "2  dear people use computer everyone agrees benef...  \n",
       "3  dear local newspaper found many expert say com...  \n",
       "4  dear know computer positive effect people comp...  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fefb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2793a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay', 'rater1_domain1', 'rater2_domain1']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b76790d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y_true, y_pred, average='macro'):\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(y_true, y_pred, average='macro'):\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred, average='macro'):\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calculate_cohen_kappa_score(y_true, y_pred):\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def print_metrics_function(y_actual, y_predictions):\n",
    "    \n",
    "    accuracy = calculate_accuracy(y_actual, y_predictions)\n",
    "    precision = calculate_precision(y_actual, y_predictions)\n",
    "    recall = calculate_recall(y_actual, y_predictions)\n",
    "    f1 = calculate_f1_score(y_actual, y_predictions)\n",
    "    kappa_score = calculate_cohen_kappa_score(y_actual, y_predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "173c38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9abcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_classifiers(classifier_name = \"logistic_regression\"):\n",
    "    \n",
    "    if classifier_name == 'logistic_regression':\n",
    "        return LogisticRegression()\n",
    "    elif classifier_name == 'random_forest_classifier':\n",
    "        return RandomForestClassifier()\n",
    "    elif classifier_name == 'adaboost_classifier':\n",
    "        return AdaBoostClassifier()\n",
    "    elif classifier_name == 'k_neighbors_classifier':\n",
    "        return KNeighborsClassifier()\n",
    "    elif classifier_name == 'support_vector_classifier':\n",
    "        return SVC()\n",
    "    else:\n",
    "        raise ValueError(f\"Classifier {classifier_name} not supported for this problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e83186",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1395cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 1]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b54299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64a8c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c372e360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 137ms/step - loss: 2.3262 - accuracy: 0.1536 - val_loss: 1.9795 - val_accuracy: 0.2157\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 3s 114ms/step - loss: 1.8205 - accuracy: 0.3401 - val_loss: 1.7840 - val_accuracy: 0.3641\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 3s 112ms/step - loss: 1.7013 - accuracy: 0.3906 - val_loss: 1.7691 - val_accuracy: 0.3641\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 3s 114ms/step - loss: 1.6189 - accuracy: 0.3913 - val_loss: 1.7384 - val_accuracy: 0.3641\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 3s 115ms/step - loss: 1.4634 - accuracy: 0.4369 - val_loss: 1.7740 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 3s 115ms/step - loss: 1.2291 - accuracy: 0.5764 - val_loss: 1.8665 - val_accuracy: 0.3277\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 3s 111ms/step - loss: 1.0500 - accuracy: 0.6143 - val_loss: 2.0600 - val_accuracy: 0.3473\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 3s 114ms/step - loss: 0.9643 - accuracy: 0.6150 - val_loss: 2.1690 - val_accuracy: 0.3501\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 3s 112ms/step - loss: 0.8991 - accuracy: 0.6557 - val_loss: 2.2979 - val_accuracy: 0.3361\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 3s 118ms/step - loss: 0.8994 - accuracy: 0.6494 - val_loss: 2.5051 - val_accuracy: 0.3501\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70b33e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35014005602240894\n",
      "Precision: 0.20081637532026358\n",
      "Recall: 0.16341059142470432\n",
      "F1-Score: 0.16172301284509846\n",
      "Cohen Kappa Score: 0.40699451586802116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.35014005602240894,\n",
       " 0.20081637532026358,\n",
       " 0.16341059142470432,\n",
       " 0.16172301284509846,\n",
       " 0.40699451586802116)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f173d",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99c57f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 2]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ba50056",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5be42453",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4b0a28fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 4s 174ms/step - loss: 1.3755 - accuracy: 0.4306 - val_loss: 1.1830 - val_accuracy: 0.4722\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 3s 115ms/step - loss: 1.1294 - accuracy: 0.5403 - val_loss: 1.1258 - val_accuracy: 0.5222\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 3s 113ms/step - loss: 1.0096 - accuracy: 0.6285 - val_loss: 1.1025 - val_accuracy: 0.5167\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 3s 116ms/step - loss: 0.8307 - accuracy: 0.7278 - val_loss: 1.1216 - val_accuracy: 0.5278\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 3s 114ms/step - loss: 0.6926 - accuracy: 0.7771 - val_loss: 1.2684 - val_accuracy: 0.5139\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 3s 116ms/step - loss: 0.5989 - accuracy: 0.8014 - val_loss: 1.2997 - val_accuracy: 0.5083\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 3s 115ms/step - loss: 0.5270 - accuracy: 0.8250 - val_loss: 1.4001 - val_accuracy: 0.4972\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 3s 114ms/step - loss: 0.4958 - accuracy: 0.8375 - val_loss: 1.5644 - val_accuracy: 0.5083\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 3s 114ms/step - loss: 0.4560 - accuracy: 0.8528 - val_loss: 1.5139 - val_accuracy: 0.5056\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 3s 116ms/step - loss: 0.4226 - accuracy: 0.8597 - val_loss: 1.6583 - val_accuracy: 0.4972\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "951bf32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49722222222222223\n",
      "Precision: 0.201593827082411\n",
      "Recall: 0.23339276617422314\n",
      "F1-Score: 0.21575757575757573\n",
      "Cohen Kappa Score: 0.2920835145715528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.49722222222222223,\n",
       " 0.201593827082411,\n",
       " 0.23339276617422314,\n",
       " 0.21575757575757573,\n",
       " 0.2920835145715528)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0e0ad",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f982270",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 3]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60f25920",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c43c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c81346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 3s 136ms/step - loss: 1.2259 - accuracy: 0.3797 - val_loss: 1.1649 - val_accuracy: 0.3786\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 2s 109ms/step - loss: 1.1500 - accuracy: 0.4029 - val_loss: 1.1770 - val_accuracy: 0.3671\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 2s 112ms/step - loss: 1.1231 - accuracy: 0.4145 - val_loss: 1.1747 - val_accuracy: 0.3728\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 2s 110ms/step - loss: 1.0869 - accuracy: 0.4471 - val_loss: 1.2099 - val_accuracy: 0.3815\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 2s 110ms/step - loss: 1.0735 - accuracy: 0.4659 - val_loss: 1.2123 - val_accuracy: 0.3815\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 2s 110ms/step - loss: 1.0505 - accuracy: 0.4536 - val_loss: 1.1806 - val_accuracy: 0.3902\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 2s 109ms/step - loss: 1.0353 - accuracy: 0.4739 - val_loss: 1.1973 - val_accuracy: 0.4017\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 2s 109ms/step - loss: 1.0215 - accuracy: 0.4710 - val_loss: 1.2122 - val_accuracy: 0.3873\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 2s 111ms/step - loss: 1.0213 - accuracy: 0.4891 - val_loss: 1.1809 - val_accuracy: 0.3671\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 2s 109ms/step - loss: 1.0312 - accuracy: 0.4630 - val_loss: 1.2134 - val_accuracy: 0.3786\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "19e5859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3786127167630058\n",
      "Precision: 0.2841512551990493\n",
      "Recall: 0.25842119628208865\n",
      "F1-Score: 0.2026565961986403\n",
      "Cohen Kappa Score: 0.0727168973036868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3786127167630058,\n",
       " 0.2841512551990493,\n",
       " 0.25842119628208865,\n",
       " 0.2026565961986403,\n",
       " 0.0727168973036868)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652711c5",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46c91e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 4]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "27d987a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bf08ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "501a718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 137ms/step - loss: 1.2776 - accuracy: 0.4576 - val_loss: 1.2027 - val_accuracy: 0.4887\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 3s 109ms/step - loss: 1.0385 - accuracy: 0.5777 - val_loss: 1.0890 - val_accuracy: 0.5141\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 3s 111ms/step - loss: 0.8871 - accuracy: 0.6123 - val_loss: 1.0506 - val_accuracy: 0.5113\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 3s 126ms/step - loss: 0.7730 - accuracy: 0.6363 - val_loss: 1.0580 - val_accuracy: 0.5085\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 3s 124ms/step - loss: 0.6968 - accuracy: 0.6956 - val_loss: 1.0602 - val_accuracy: 0.5226\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 3s 115ms/step - loss: 0.6456 - accuracy: 0.7422 - val_loss: 1.0816 - val_accuracy: 0.5339\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 3s 114ms/step - loss: 0.6095 - accuracy: 0.7493 - val_loss: 1.1234 - val_accuracy: 0.5254\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 3s 128ms/step - loss: 0.5527 - accuracy: 0.7677 - val_loss: 1.0840 - val_accuracy: 0.5311\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 3s 109ms/step - loss: 0.5063 - accuracy: 0.7782 - val_loss: 1.0754 - val_accuracy: 0.5311\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 3s 113ms/step - loss: 0.4845 - accuracy: 0.7839 - val_loss: 1.1174 - val_accuracy: 0.5085\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "602d4bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5084745762711864\n",
      "Precision: 0.3962862318840579\n",
      "Recall: 0.44466403162055335\n",
      "F1-Score: 0.41455028425474094\n",
      "Cohen Kappa Score: 0.617012644533099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5084745762711864,\n",
       " 0.3962862318840579,\n",
       " 0.44466403162055335,\n",
       " 0.41455028425474094,\n",
       " 0.617012644533099)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f359239",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32e4001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 5]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4d21ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e5a05b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d39ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 143ms/step - loss: 1.3470 - accuracy: 0.3636 - val_loss: 1.1741 - val_accuracy: 0.4543\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 3s 112ms/step - loss: 1.0596 - accuracy: 0.5076 - val_loss: 1.1027 - val_accuracy: 0.4681\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 3s 111ms/step - loss: 0.9142 - accuracy: 0.6191 - val_loss: 1.1743 - val_accuracy: 0.4571\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 3s 111ms/step - loss: 0.7932 - accuracy: 0.6620 - val_loss: 1.2074 - val_accuracy: 0.4765\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 3s 112ms/step - loss: 0.6963 - accuracy: 0.7161 - val_loss: 1.2890 - val_accuracy: 0.4681\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 3s 112ms/step - loss: 0.6256 - accuracy: 0.7625 - val_loss: 1.3502 - val_accuracy: 0.4903\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 3s 112ms/step - loss: 0.5670 - accuracy: 0.7798 - val_loss: 1.3725 - val_accuracy: 0.4986\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 3s 110ms/step - loss: 0.5223 - accuracy: 0.8089 - val_loss: 1.4605 - val_accuracy: 0.4792\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 3s 111ms/step - loss: 0.4698 - accuracy: 0.8331 - val_loss: 1.6040 - val_accuracy: 0.4515\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 3s 111ms/step - loss: 0.4464 - accuracy: 0.8435 - val_loss: 1.6730 - val_accuracy: 0.4432\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b18d90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44321329639889195\n",
      "Precision: 0.3655787735683871\n",
      "Recall: 0.362267803966723\n",
      "F1-Score: 0.35922234068950837\n",
      "Cohen Kappa Score: 0.5484591182886587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.44321329639889195,\n",
       " 0.3655787735683871,\n",
       " 0.362267803966723,\n",
       " 0.35922234068950837,\n",
       " 0.5484591182886587)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823e73a",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6bc7f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 6]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "236f5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e7dde3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1228bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 3s 138ms/step - loss: 1.3828 - accuracy: 0.4326 - val_loss: 1.2696 - val_accuracy: 0.4806\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 3s 134ms/step - loss: 1.3105 - accuracy: 0.4563 - val_loss: 1.2563 - val_accuracy: 0.4806\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 3s 129ms/step - loss: 1.2523 - accuracy: 0.4903 - val_loss: 1.2689 - val_accuracy: 0.4611\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 3s 128ms/step - loss: 1.1624 - accuracy: 0.5451 - val_loss: 1.3187 - val_accuracy: 0.4222\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 3s 122ms/step - loss: 1.0896 - accuracy: 0.5833 - val_loss: 1.3162 - val_accuracy: 0.4694\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 3s 117ms/step - loss: 1.0196 - accuracy: 0.5951 - val_loss: 1.3759 - val_accuracy: 0.4472\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 3s 113ms/step - loss: 0.9533 - accuracy: 0.6375 - val_loss: 1.4337 - val_accuracy: 0.4278\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 3s 116ms/step - loss: 0.8964 - accuracy: 0.6583 - val_loss: 1.4738 - val_accuracy: 0.4528\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 3s 116ms/step - loss: 0.8706 - accuracy: 0.6576 - val_loss: 1.4716 - val_accuracy: 0.4333\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 3s 115ms/step - loss: 0.8248 - accuracy: 0.6757 - val_loss: 1.4666 - val_accuracy: 0.4444\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7837719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4444444444444444\n",
      "Precision: 0.22929596719070405\n",
      "Recall: 0.24350686885368966\n",
      "F1-Score: 0.2341949175676111\n",
      "Cohen Kappa Score: 0.2466666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4444444444444444,\n",
       " 0.22929596719070405,\n",
       " 0.24350686885368966,\n",
       " 0.2341949175676111,\n",
       " 0.2466666666666667)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8178002",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4e427cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 7]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "02643071",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "47b6ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a1dbe49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 3.0773 - accuracy: 0.1267 - val_loss: 2.9478 - val_accuracy: 0.0924\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 2.9029 - accuracy: 0.1355 - val_loss: 2.8739 - val_accuracy: 0.0924\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 2.8022 - accuracy: 0.1618 - val_loss: 2.8989 - val_accuracy: 0.0987\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 2s 114ms/step - loss: 2.6656 - accuracy: 0.1936 - val_loss: 2.9453 - val_accuracy: 0.1115\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 2s 115ms/step - loss: 2.5159 - accuracy: 0.2072 - val_loss: 3.0612 - val_accuracy: 0.1083\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 2s 115ms/step - loss: 2.4241 - accuracy: 0.2311 - val_loss: 3.2061 - val_accuracy: 0.1115\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 2.3595 - accuracy: 0.2446 - val_loss: 3.2988 - val_accuracy: 0.0924\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 2.2723 - accuracy: 0.2510 - val_loss: 3.5321 - val_accuracy: 0.0892\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 3s 125ms/step - loss: 2.2150 - accuracy: 0.2741 - val_loss: 3.5136 - val_accuracy: 0.1178\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 2s 121ms/step - loss: 2.1696 - accuracy: 0.2861 - val_loss: 3.7134 - val_accuracy: 0.0987\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d6bbd0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09872611464968153\n",
      "Precision: 0.01921364667907223\n",
      "Recall: 0.04673705759287174\n",
      "F1-Score: 0.026198926200366585\n",
      "Cohen Kappa Score: -0.0004539280368134335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09872611464968153,\n",
       " 0.01921364667907223,\n",
       " 0.04673705759287174,\n",
       " 0.026198926200366585,\n",
       " -0.0004539280368134335)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5557edf",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f09b76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 8]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1921a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "beede2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b48163f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 2s 175ms/step - loss: 4.0378 - accuracy: 0.0433 - val_loss: 3.8350 - val_accuracy: 0.0345\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 3.5940 - accuracy: 0.0450 - val_loss: 3.2860 - val_accuracy: 0.0345\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 3.0937 - accuracy: 0.1765 - val_loss: 3.0136 - val_accuracy: 0.2000\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 2.9118 - accuracy: 0.2284 - val_loss: 3.0095 - val_accuracy: 0.2000\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 2.8718 - accuracy: 0.2284 - val_loss: 2.9918 - val_accuracy: 0.2000\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 2.8247 - accuracy: 0.2284 - val_loss: 2.9990 - val_accuracy: 0.2000\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 2.7716 - accuracy: 0.2284 - val_loss: 3.0451 - val_accuracy: 0.2000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 2.6956 - accuracy: 0.2578 - val_loss: 3.0309 - val_accuracy: 0.2000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 2.6100 - accuracy: 0.2284 - val_loss: 3.0520 - val_accuracy: 0.1586\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 2.5398 - accuracy: 0.2803 - val_loss: 3.0677 - val_accuracy: 0.1448\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 512\n",
    "lstm_units = 4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = len(y_train[0])\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   # Dropout(0.2),\n",
    "                   # LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(num_classes, activation = \"softmax\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(padded_sequences_train, y_train, batch_size = batch_size, validation_data = (padded_sequences_test, y_test),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ffa0459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.14482758620689656\n",
      "Precision: 0.010169977081741789\n",
      "Recall: 0.03807471264367816\n",
      "F1-Score: 0.015683962264150943\n",
      "Cohen Kappa Score: 0.21483088294368113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.14482758620689656,\n",
       " 0.010169977081741789,\n",
       " 0.03807471264367816,\n",
       " 0.015683962264150943,\n",
       " 0.21483088294368113)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
