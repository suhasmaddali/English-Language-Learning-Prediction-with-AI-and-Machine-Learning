{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5815cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f662f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95780a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['full_text', 'text_id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72f40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error (MSE) between the true and predicted values\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse\n",
    "\n",
    "def calc_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean squared error (MSE) between the true and predicted values\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse\n",
    "\n",
    "def calc_mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean absolute error (MAE) between the true and predicted values\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return mae\n",
    "\n",
    "def calc_rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared error (RMSE) between the true and predicted values\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def calc_mape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mean absolute percentage error (MAPE) between the true and predicted values\n",
    "    \"\"\"\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return mape\n",
    "\n",
    "def calc_r2_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the R2 score between the true and predicted values\n",
    "    \"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57dc354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print MSE\n",
    "def print_metrics_function(y_actual, y_predictions):\n",
    "    \n",
    "    # Calculate and print MSE\n",
    "    mse = calc_mse(y_actual, y_predictions)\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "    # Calculate and print RMSE\n",
    "    rmse = calc_rmse(y_actual, y_predictions)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    \n",
    "    # Calculate and print MAE\n",
    "    mae = calc_mae(y_actual, y_predictions)\n",
    "    print(\"MAE:\", mae)\n",
    "\n",
    "    # Calculate and print MAPE\n",
    "    mape = calc_mape(y_actual, y_predictions)\n",
    "    print(\"MAPE:\", mape)\n",
    "\n",
    "    # Calculate and print R2 score\n",
    "    r2 = calc_r2_score(y_actual, y_predictions)\n",
    "    print(\"R2 Score:\", r2)\n",
    "    \n",
    "    return mse, rmse, mae, mape, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a763325",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohesion = df['cohesion']\n",
    "syntax = df['syntax']\n",
    "vocabulary = df['vocabulary']\n",
    "phraseology = df['phraseology']\n",
    "grammar = df['grammar']\n",
    "conventions = df['conventions']\n",
    "\n",
    "preprocessed_text = df['preprocessed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d2386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed_text\n",
    "y_cohesion = cohesion\n",
    "y_syntax = syntax\n",
    "y_vocabulary = vocabulary\n",
    "y_phraseology = phraseology\n",
    "y_grammar = grammar\n",
    "y_conventions = conventions\n",
    "\n",
    "X_train, X_test, y_train_cohesion, y_test_cohesion = train_test_split(X, y_cohesion, \n",
    "                                                                     shuffle = True, \n",
    "                                                                     random_state = 101, \n",
    "                                                                     test_size = 0.2)\n",
    "\n",
    "X_train, X_test, y_train_syntax, y_test_syntax = train_test_split(X, y_syntax, \n",
    "                                                                     shuffle = True, \n",
    "                                                                     random_state = 101, \n",
    "                                                                     test_size = 0.2)\n",
    "\n",
    "X_train, X_test, y_train_vocabulary, y_test_vocabulary = train_test_split(X, y_vocabulary, \n",
    "                                                                     shuffle = True, \n",
    "                                                                     random_state = 101, \n",
    "                                                                     test_size = 0.2)\n",
    "\n",
    "X_train, X_test, y_train_phraseology, y_test_phraseology = train_test_split(X, y_phraseology, \n",
    "                                                                     shuffle = True, \n",
    "                                                                     random_state = 101, \n",
    "                                                                     test_size = 0.2)\n",
    "\n",
    "X_train, X_test, y_train_grammar, y_test_grammar = train_test_split(X, y_grammar, \n",
    "                                                                     shuffle = True, \n",
    "                                                                     random_state = 101, \n",
    "                                                                     test_size = 0.2)\n",
    "\n",
    "X_train, X_test, y_train_conventions, y_test_conventions = train_test_split(X, y_conventions, \n",
    "                                                                     shuffle = True, \n",
    "                                                                     random_state = 101, \n",
    "                                                                     test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b54299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64a8c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 1000\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_sequence_len,\n",
    "                                      padding = 'post',\n",
    "                                      truncating = 'post')\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_sequence_len,\n",
    "                                     padding = 'post', \n",
    "                                     truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e83186",
   "metadata": {},
   "source": [
    "### Cohesion Score Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "lstm_units = 32\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   Dropout(0.2),\n",
    "                   LSTM(lstm_units, dropout = 0.2),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(1, activation = \"linear\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "model.fit(padded_sequences_train, y_train_cohesion, batch_size = batch_size, validation_data = (padded_sequences_test, y_test_cohesion),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b299e",
   "metadata": {},
   "source": [
    "### Syntax Score Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30b27f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 5s 97ms/step - loss: 0.6677 - mean_absolute_error: 0.6677 - val_loss: 0.5071 - val_mean_absolute_error: 0.5071\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 4s 82ms/step - loss: 0.5040 - mean_absolute_error: 0.5040 - val_loss: 0.5247 - val_mean_absolute_error: 0.5247\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 4s 82ms/step - loss: 0.4962 - mean_absolute_error: 0.4962 - val_loss: 0.5133 - val_mean_absolute_error: 0.5133\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 4s 84ms/step - loss: 0.4948 - mean_absolute_error: 0.4948 - val_loss: 0.5022 - val_mean_absolute_error: 0.5022\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 4s 83ms/step - loss: 0.4999 - mean_absolute_error: 0.4999 - val_loss: 0.5302 - val_mean_absolute_error: 0.5302\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 4s 83ms/step - loss: 0.4959 - mean_absolute_error: 0.4959 - val_loss: 0.5180 - val_mean_absolute_error: 0.5180\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 4s 83ms/step - loss: 0.4985 - mean_absolute_error: 0.4985 - val_loss: 0.5093 - val_mean_absolute_error: 0.5093\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 4s 83ms/step - loss: 0.4976 - mean_absolute_error: 0.4976 - val_loss: 0.5340 - val_mean_absolute_error: 0.5340\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 4s 83ms/step - loss: 0.5111 - mean_absolute_error: 0.5111 - val_loss: 0.5282 - val_mean_absolute_error: 0.5282\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 4s 84ms/step - loss: 0.4969 - mean_absolute_error: 0.4969 - val_loss: 0.5162 - val_mean_absolute_error: 0.5162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e80c9a970>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "lstm_units = 32\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   Dropout(0.2),\n",
    "                   LSTM(lstm_units, dropout = 0.2),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(1, activation = \"linear\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "model.fit(padded_sequences_train, y_train_syntax, batch_size = batch_size, validation_data = (padded_sequences_test, y_test_syntax),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6ffcb",
   "metadata": {},
   "source": [
    "### Vocabulary Score Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b7939f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 5s 100ms/step - loss: 0.6526 - mean_absolute_error: 0.6526 - val_loss: 0.4837 - val_mean_absolute_error: 0.4837\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 4s 82ms/step - loss: 0.4726 - mean_absolute_error: 0.4726 - val_loss: 0.4801 - val_mean_absolute_error: 0.4801\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 4s 82ms/step - loss: 0.4712 - mean_absolute_error: 0.4712 - val_loss: 0.4730 - val_mean_absolute_error: 0.4730\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 4s 83ms/step - loss: 0.4520 - mean_absolute_error: 0.4520 - val_loss: 0.4867 - val_mean_absolute_error: 0.4867\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 4s 86ms/step - loss: 0.4494 - mean_absolute_error: 0.4494 - val_loss: 0.4702 - val_mean_absolute_error: 0.4702\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 4s 84ms/step - loss: 0.4459 - mean_absolute_error: 0.4459 - val_loss: 0.4696 - val_mean_absolute_error: 0.4696\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 4s 86ms/step - loss: 0.4498 - mean_absolute_error: 0.4498 - val_loss: 0.4787 - val_mean_absolute_error: 0.4787\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.4479 - mean_absolute_error: 0.4479 - val_loss: 0.4724 - val_mean_absolute_error: 0.4724\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.4503 - mean_absolute_error: 0.4503 - val_loss: 0.4753 - val_mean_absolute_error: 0.4753\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 4s 89ms/step - loss: 0.4461 - mean_absolute_error: 0.4461 - val_loss: 0.4824 - val_mean_absolute_error: 0.4824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e91c672b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "lstm_units = 32\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   Dropout(0.2),\n",
    "                   LSTM(lstm_units, dropout = 0.2),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(1, activation = \"linear\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "model.fit(padded_sequences_train, y_train_vocabulary, batch_size = batch_size, validation_data = (padded_sequences_test, y_test_vocabulary),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54912aac",
   "metadata": {},
   "source": [
    "### Phraseology Score Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01270c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 5s 105ms/step - loss: 0.7298 - mean_absolute_error: 0.7298 - val_loss: 0.5256 - val_mean_absolute_error: 0.5256\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 4s 84ms/step - loss: 0.5238 - mean_absolute_error: 0.5238 - val_loss: 0.5246 - val_mean_absolute_error: 0.5246\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.5174 - mean_absolute_error: 0.5174 - val_loss: 0.5259 - val_mean_absolute_error: 0.5259\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 4s 86ms/step - loss: 0.5300 - mean_absolute_error: 0.5300 - val_loss: 0.5440 - val_mean_absolute_error: 0.5440\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 4s 90ms/step - loss: 0.5360 - mean_absolute_error: 0.5360 - val_loss: 0.5244 - val_mean_absolute_error: 0.5244\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 4s 88ms/step - loss: 0.5161 - mean_absolute_error: 0.5161 - val_loss: 0.5489 - val_mean_absolute_error: 0.5489\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 4s 89ms/step - loss: 0.5232 - mean_absolute_error: 0.5232 - val_loss: 0.5271 - val_mean_absolute_error: 0.5271\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 4s 90ms/step - loss: 0.5194 - mean_absolute_error: 0.5194 - val_loss: 0.5362 - val_mean_absolute_error: 0.5362\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 4s 90ms/step - loss: 0.5250 - mean_absolute_error: 0.5250 - val_loss: 0.5318 - val_mean_absolute_error: 0.5318\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 4s 90ms/step - loss: 0.5229 - mean_absolute_error: 0.5229 - val_loss: 0.5267 - val_mean_absolute_error: 0.5267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e8a4f4eb0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "lstm_units = 32\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   Dropout(0.2),\n",
    "                   LSTM(lstm_units, dropout = 0.2),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(1, activation = \"linear\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "model.fit(padded_sequences_train, y_train_phraseology, batch_size = batch_size, validation_data = (padded_sequences_test, y_test_phraseology),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce404ed",
   "metadata": {},
   "source": [
    "### Grammar Score Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "def528b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 5s 107ms/step - loss: 0.7180 - mean_absolute_error: 0.7180 - val_loss: 0.5876 - val_mean_absolute_error: 0.5876\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.5618 - mean_absolute_error: 0.5618 - val_loss: 0.5741 - val_mean_absolute_error: 0.5741\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 4s 89ms/step - loss: 0.5649 - mean_absolute_error: 0.5649 - val_loss: 0.5900 - val_mean_absolute_error: 0.5900\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 4s 90ms/step - loss: 0.5728 - mean_absolute_error: 0.5728 - val_loss: 0.5932 - val_mean_absolute_error: 0.5932\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 5s 99ms/step - loss: 0.5671 - mean_absolute_error: 0.5671 - val_loss: 0.5945 - val_mean_absolute_error: 0.5945\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 5s 96ms/step - loss: 0.5872 - mean_absolute_error: 0.5872 - val_loss: 0.5714 - val_mean_absolute_error: 0.5714\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 5s 97ms/step - loss: 0.5555 - mean_absolute_error: 0.5555 - val_loss: 0.5729 - val_mean_absolute_error: 0.5729\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 5s 96ms/step - loss: 0.5567 - mean_absolute_error: 0.5567 - val_loss: 0.5937 - val_mean_absolute_error: 0.5937\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 5s 94ms/step - loss: 0.5612 - mean_absolute_error: 0.5612 - val_loss: 0.5695 - val_mean_absolute_error: 0.5695\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 5s 95ms/step - loss: 0.5626 - mean_absolute_error: 0.5626 - val_loss: 0.5726 - val_mean_absolute_error: 0.5726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e9955c400>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "lstm_units = 32\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   Dropout(0.2),\n",
    "                   LSTM(lstm_units, dropout = 0.2),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(1, activation = \"linear\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "model.fit(padded_sequences_train, y_train_grammar, batch_size = batch_size, validation_data = (padded_sequences_test, y_test_grammar),\n",
    "         epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e8f1e",
   "metadata": {},
   "source": [
    "### Convention Score Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a2f2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 5s 109ms/step - loss: 0.7577 - mean_absolute_error: 0.7577 - val_loss: 0.5377 - val_mean_absolute_error: 0.5377\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.5289 - mean_absolute_error: 0.5289 - val_loss: 0.5463 - val_mean_absolute_error: 0.5463\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 4s 86ms/step - loss: 0.5271 - mean_absolute_error: 0.5271 - val_loss: 0.5496 - val_mean_absolute_error: 0.5496\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.5257 - mean_absolute_error: 0.5257 - val_loss: 0.5399 - val_mean_absolute_error: 0.5399\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 4s 89ms/step - loss: 0.5175 - mean_absolute_error: 0.5175 - val_loss: 0.5436 - val_mean_absolute_error: 0.5436\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 4s 90ms/step - loss: 0.5263 - mean_absolute_error: 0.5263 - val_loss: 0.5474 - val_mean_absolute_error: 0.5474\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 4s 89ms/step - loss: 0.5296 - mean_absolute_error: 0.5296 - val_loss: 0.5401 - val_mean_absolute_error: 0.5401\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.5172 - mean_absolute_error: 0.5172 - val_loss: 0.5390 - val_mean_absolute_error: 0.5390\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 4s 86ms/step - loss: 0.5177 - mean_absolute_error: 0.5177 - val_loss: 0.5448 - val_mean_absolute_error: 0.5448\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 4s 87ms/step - loss: 0.5217 - mean_absolute_error: 0.5217 - val_loss: 0.5418 - val_mean_absolute_error: 0.5418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e9f4b0040>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "lstm_units = 32\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential([Embedding(vocab_size, embedding_dim, input_length = max_sequence_len),\n",
    "                   LSTM(lstm_units, dropout = 0.2, return_sequences = True),\n",
    "                   Dropout(0.2),\n",
    "                   LSTM(lstm_units, dropout = 0.2),\n",
    "                    Dense(10, activation = \"relu\"),\n",
    "                   Dense(1, activation = \"linear\")])\n",
    "\n",
    "optimizer = Adam(lr = 0.01)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "model.fit(padded_sequences_train, y_train_conventions, batch_size = batch_size, validation_data = (padded_sequences_test, y_test_conventions),\n",
    "         epochs = epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
