{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "40756fa7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1471d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "67ad8f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>pos_ratios</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>386</td>\n",
       "      <td>1875</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.031088082901554404, 'JJ': 0.05181347...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>0.385613</td>\n",
       "      <td>dear local newspaper think effect computer peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>2288</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.03879310344827586, ',': 0.0258620689...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.613167</td>\n",
       "      <td>dear believe using computer benefit u many way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>1541</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.04153354632587859, ',': 0.0287539936...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340393</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>dear people use computer everyone agrees benef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611</td>\n",
       "      <td>3165</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.11620294599018004, ',': 0.0212765957...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.441795</td>\n",
       "      <td>dear local newspaper found many expert say com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>517</td>\n",
       "      <td>2569</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.017408123791102514, ',': 0.025145067...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>dear know computer positive effect people comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  word_len  chars_len  \\\n",
       "0             NaN             NaN            NaN  ...       386       1875   \n",
       "1             NaN             NaN            NaN  ...       464       2288   \n",
       "2             NaN             NaN            NaN  ...       313       1541   \n",
       "3             NaN             NaN            NaN  ...       611       3165   \n",
       "4             NaN             NaN            NaN  ...       517       2569   \n",
       "\n",
       "   avg_word_length  avg_sentence_length  \\\n",
       "0         3.984456                  1.0   \n",
       "1         4.030172                  1.0   \n",
       "2         4.035144                  1.0   \n",
       "3         4.328969                  1.0   \n",
       "4         4.071567                  1.0   \n",
       "\n",
       "                                          pos_ratios  num_sentences  \\\n",
       "0  {'NNP': 0.031088082901554404, 'JJ': 0.05181347...             16   \n",
       "1  {'NNP': 0.03879310344827586, ',': 0.0258620689...             20   \n",
       "2  {'NNP': 0.04153354632587859, ',': 0.0287539936...             14   \n",
       "3  {'NNP': 0.11620294599018004, ',': 0.0212765957...             27   \n",
       "4  {'NNP': 0.017408123791102514, ',': 0.025145067...             30   \n",
       "\n",
       "   num_paragraphs  sentiment_polariy  sentiment_subjectivity  \\\n",
       "0               1           0.310471                0.385613   \n",
       "1               1           0.274000                0.613167   \n",
       "2               1           0.340393                0.498657   \n",
       "3               1           0.266828                0.441795   \n",
       "4               1           0.199684                0.485814   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear local newspaper think effect computer peo...  \n",
       "1  dear believe using computer benefit u many way...  \n",
       "2  dear people use computer everyone agrees benef...  \n",
       "3  dear local newspaper found many expert say com...  \n",
       "4  dear know computer positive effect people comp...  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "725b81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fdd0fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay', 'rater1_domain1', 'rater2_domain1']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b76790d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y_true, y_pred, average='macro'):\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(y_true, y_pred, average='macro'):\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred, average='macro'):\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calculate_cohen_kappa_score(y_true, y_pred):\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def print_metrics_function(y_actual, y_predictions):\n",
    "    \n",
    "    accuracy = calculate_accuracy(y_actual, y_predictions)\n",
    "    precision = calculate_precision(y_actual, y_predictions)\n",
    "    recall = calculate_recall(y_actual, y_predictions)\n",
    "    f1 = calculate_f1_score(y_actual, y_predictions)\n",
    "    kappa_score = calculate_cohen_kappa_score(y_actual, y_predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "173c38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a75f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_classifiers(classifier_name = \"logistic_regression\"):\n",
    "    \n",
    "    if classifier_name == 'logistic_regression':\n",
    "        return LogisticRegression()\n",
    "    elif classifier_name == 'decision_tree_classifier':\n",
    "        return DecisionTreeClassifier()\n",
    "    elif classifier_name == 'random_forest_classifier':\n",
    "        return RandomForestClassifier()\n",
    "    elif classifier_name == 'adaboost_classifier':\n",
    "        return AdaBoostClassifier()\n",
    "    elif classifier_name == 'k_neighbors_classifier':\n",
    "        return KNeighborsClassifier()\n",
    "    elif classifier_name == 'support_vector_classifier':\n",
    "        return SVC()\n",
    "    else:\n",
    "        raise ValueError(f\"Classifier {classifier_name} not supported for this problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2344ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 1]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9d2dd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrector(tokens):\n",
    "    spell_checker = SpellChecker()\n",
    "    correct_tokens = []\n",
    "    for token in tqdm(tokens):\n",
    "        if spell_checker.correction(token.lower()):\n",
    "            correct_tokens.append(spell_checker.correction(token.lower()))\n",
    "        else:\n",
    "            correct_tokens.append(token.lower())\n",
    "    \n",
    "    return ' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "093a9056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>1</td>\n",
       "      <td>431</td>\n",
       "      <td>1993</td>\n",
       "      <td>3.781903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096094</td>\n",
       "      <td>0.511334</td>\n",
       "      <td>dear local people using computer year good hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>1498</td>\n",
       "      <td>4.283737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066142</td>\n",
       "      <td>0.514918</td>\n",
       "      <td>dear newspaper believe computer positive affec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>2724</td>\n",
       "      <td>4.037770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.262030</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>people agree computer make life le complicated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>2259</td>\n",
       "      <td>3.837838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.099660</td>\n",
       "      <td>0.426994</td>\n",
       "      <td>dear world changed much better technology beco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>1</td>\n",
       "      <td>488</td>\n",
       "      <td>2369</td>\n",
       "      <td>3.954918</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053060</td>\n",
       "      <td>0.429295</td>\n",
       "      <td>technology growing changing rapidly look apple...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_set  word_len  chars_len  avg_word_length  avg_sentence_length  \\\n",
       "1346          1       431       1993         3.781903                  1.0   \n",
       "1349          1       289       1498         4.283737                  1.0   \n",
       "7             1       556       2724         4.037770                  1.0   \n",
       "1251          1       481       2259         3.837838                  1.0   \n",
       "661           1       488       2369         3.954918                  1.0   \n",
       "\n",
       "      num_sentences  num_paragraphs  sentiment_polariy  \\\n",
       "1346             37               1           0.096094   \n",
       "1349             15               1           0.066142   \n",
       "7                39               1           0.262030   \n",
       "1251             31               1           0.099660   \n",
       "661              26               1           0.053060   \n",
       "\n",
       "      sentiment_subjectivity  \\\n",
       "1346                0.511334   \n",
       "1349                0.514918   \n",
       "7                   0.574500   \n",
       "1251                0.426994   \n",
       "661                 0.429295   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "1346  dear local people using computer year good hea...  \n",
       "1349  dear newspaper believe computer positive affec...  \n",
       "7     people agree computer make life le complicated...  \n",
       "1251  dear world changed much better technology beco...  \n",
       "661   technology growing changing rapidly look apple...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eca450",
   "metadata": {},
   "source": [
    "### BERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5c4097a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77f9db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:16<00:00,  5.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:04<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce28fa",
   "metadata": {},
   "source": [
    "#### Adding new features from feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3be90f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aca81955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_function(input_data, output_data, ml_model = \"logistic_regression\",\n",
    "                             print_results = True, n_folds = 5, return_kappa_scores = True):\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    kappa_score_list = []\n",
    "    \n",
    "    k_fold = KFold(n_splits=n_folds, shuffle=True, random_state=101)\n",
    "    X = pd.DataFrame(input_data.numpy()).copy()\n",
    "    y = output_data.copy()\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(k_fold.split(X, y)):\n",
    "        \n",
    "        train_embeddings, train_y = X.iloc[train_indices], y.iloc[train_indices]\n",
    "        val_embeddings, val_y = X.iloc[val_indices], y.iloc[val_indices]\n",
    "        model = choose_classifiers(ml_model)\n",
    "        model.fit(train_embeddings, train_y)\n",
    "        y_predictions = model.predict(val_embeddings)\n",
    "        accuracy_logistic_reg, precision_logistic_reg, recall_logistic_reg, f1_logistic_reg, kappa_score_logistic_reg = print_metrics_function(val_y, y_predictions)\n",
    "        accuracy_list.append(accuracy_logistic_reg)\n",
    "        precision_list.append(precision_logistic_reg)\n",
    "        recall_list.append(recall_logistic_reg)\n",
    "        f1_list.append(f1_logistic_reg)\n",
    "        kappa_score_list.append(kappa_score_logistic_reg)\n",
    "    \n",
    "    if print_results:\n",
    "        \n",
    "        print(\"Accuracy: {:.4f}\".format(np.max(accuracy_list)))\n",
    "        print(\"Precision: {:.4f}\".format(np.max(precision_list)))\n",
    "        print(\"Recall: {:.4f}\".format(np.max(recall_list)))\n",
    "        print(\"F1 score: {:.4f}\".format(np.max(f1_list)))\n",
    "        print(\"Kappa score: {:.4f}\".format(np.max(kappa_score_list)))\n",
    "        \n",
    "    if return_kappa_scores:\n",
    "        \n",
    "        return round(np.max(kappa_score_list), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "992dfa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.3497\n",
      "Precision: 0.2993\n",
      "Recall: 0.3225\n",
      "F1 score: 0.2507\n",
      "Kappa score: 0.5091\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.5439\n",
      "Precision: 0.4402\n",
      "Recall: 0.3871\n",
      "F1 score: 0.3864\n",
      "Kappa score: 0.7710\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5018\n",
      "Precision: 0.1932\n",
      "Recall: 0.2654\n",
      "F1 score: 0.2188\n",
      "Kappa score: 0.6589\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.2175\n",
      "Precision: 0.2300\n",
      "Recall: 0.1554\n",
      "F1 score: 0.0747\n",
      "Kappa score: 0.3542\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4667\n",
      "Precision: 0.0874\n",
      "Recall: 0.1294\n",
      "F1 score: 0.0983\n",
      "Kappa score: 0.3178\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set1 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ab54c",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cb0492bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 2]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f43dceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:16<00:00,  5.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:04<00:00,  5.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1efc10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7c05c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5694\n",
      "Precision: 0.5171\n",
      "Recall: 0.3588\n",
      "F1 score: 0.3571\n",
      "Kappa score: 0.4446\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6944\n",
      "Precision: 0.6760\n",
      "Recall: 0.4832\n",
      "F1 score: 0.5245\n",
      "Kappa score: 0.6437\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.6736\n",
      "Precision: 0.3329\n",
      "Recall: 0.3542\n",
      "F1 score: 0.3427\n",
      "Kappa score: 0.6436\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.4688\n",
      "Precision: 0.2525\n",
      "Recall: 0.2721\n",
      "F1 score: 0.2144\n",
      "Kappa score: 0.2815\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5903\n",
      "Precision: 0.2265\n",
      "Recall: 0.2702\n",
      "F1 score: 0.2461\n",
      "Kappa score: 0.4229\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set2 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed2079",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ed773749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 3]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a4de8bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:12<00:00,  7.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:02<00:00, 10.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3abd8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "55f518ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.4964\n",
      "Precision: 0.4359\n",
      "Recall: 0.4901\n",
      "F1 score: 0.4311\n",
      "Kappa score: 0.3684\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6993\n",
      "Precision: 0.5909\n",
      "Recall: 0.5370\n",
      "F1 score: 0.5306\n",
      "Kappa score: 0.7105\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.6232\n",
      "Precision: 0.5126\n",
      "Recall: 0.4829\n",
      "F1 score: 0.4752\n",
      "Kappa score: 0.6485\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.3949\n",
      "Precision: 0.2581\n",
      "Recall: 0.3307\n",
      "F1 score: 0.2213\n",
      "Kappa score: 0.1335\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.6558\n",
      "Precision: 0.4905\n",
      "Recall: 0.5016\n",
      "F1 score: 0.4954\n",
      "Kappa score: 0.6172\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set3 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725c1c9",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4c2f451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 4]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8c3e350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 89/89 [00:12<00:00,  7.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00,  9.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "728ac2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "34d527fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5689\n",
      "Precision: 0.5374\n",
      "Recall: 0.4994\n",
      "F1 score: 0.5047\n",
      "Kappa score: 0.5803\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6608\n",
      "Precision: 0.6798\n",
      "Recall: 0.6373\n",
      "F1 score: 0.6494\n",
      "Kappa score: 0.7593\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5866\n",
      "Precision: 0.5825\n",
      "Recall: 0.5873\n",
      "F1 score: 0.5833\n",
      "Kappa score: 0.7252\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.3958\n",
      "Precision: 0.3343\n",
      "Recall: 0.3229\n",
      "F1 score: 0.2577\n",
      "Kappa score: 0.2575\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5689\n",
      "Precision: 0.5089\n",
      "Recall: 0.4617\n",
      "F1 score: 0.4219\n",
      "Kappa score: 0.5278\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set4 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdff4e",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b23b57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 5]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e3e002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:13<00:00,  6.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00,  9.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0e0b5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0976af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5174\n",
      "Precision: 0.4736\n",
      "Recall: 0.5706\n",
      "F1 score: 0.5041\n",
      "Kappa score: 0.6492\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6747\n",
      "Precision: 0.5660\n",
      "Recall: 0.5481\n",
      "F1 score: 0.5519\n",
      "Kappa score: 0.8100\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5779\n",
      "Precision: 0.4298\n",
      "Recall: 0.4156\n",
      "F1 score: 0.3528\n",
      "Kappa score: 0.7038\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.3529\n",
      "Precision: 0.2306\n",
      "Recall: 0.2856\n",
      "F1 score: 0.2144\n",
      "Kappa score: 0.3718\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5347\n",
      "Precision: 0.3899\n",
      "Recall: 0.3175\n",
      "F1 score: 0.2555\n",
      "Kappa score: 0.5373\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set5 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15025161",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8dfb16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 6]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c9c4d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:15<00:00,  5.73it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:03<00:00,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "02467133",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e3e2cd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.4965\n",
      "Precision: 0.4517\n",
      "Recall: 0.4603\n",
      "F1 score: 0.4189\n",
      "Kappa score: 0.6412\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6910\n",
      "Precision: 0.6563\n",
      "Recall: 0.5260\n",
      "F1 score: 0.5330\n",
      "Kappa score: 0.7439\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5903\n",
      "Precision: 0.3707\n",
      "Recall: 0.3682\n",
      "F1 score: 0.3382\n",
      "Kappa score: 0.6087\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.3785\n",
      "Precision: 0.4553\n",
      "Recall: 0.3875\n",
      "F1 score: 0.3149\n",
      "Kappa score: 0.5403\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5625\n",
      "Precision: 0.5762\n",
      "Recall: 0.2616\n",
      "F1 score: 0.2219\n",
      "Kappa score: 0.3700\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set6 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4aed3a",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e32874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 7]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "633cd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:14<00:00,  5.38it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  7.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc520228",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "edc4e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.1036\n",
      "Precision: 0.1107\n",
      "Recall: 0.0938\n",
      "F1 score: 0.0807\n",
      "Kappa score: 0.3514\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.1992\n",
      "Precision: 0.1208\n",
      "Recall: 0.1205\n",
      "F1 score: 0.1096\n",
      "Kappa score: 0.6670\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.1992\n",
      "Precision: 0.0217\n",
      "Recall: 0.0895\n",
      "F1 score: 0.0343\n",
      "Kappa score: 0.4962\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.0438\n",
      "Precision: 0.0702\n",
      "Recall: 0.0629\n",
      "F1 score: 0.0238\n",
      "Kappa score: 0.1866\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.1793\n",
      "Precision: 0.0231\n",
      "Recall: 0.0852\n",
      "F1 score: 0.0335\n",
      "Kappa score: 0.3814\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set7 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0a9a2",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "51fd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 8]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9f42e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00,  5.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(bert_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "be0f61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cbd7cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.1897\n",
      "Precision: 0.0665\n",
      "Recall: 0.0948\n",
      "F1 score: 0.0763\n",
      "Kappa score: 0.3588\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.2696\n",
      "Precision: 0.0785\n",
      "Recall: 0.0661\n",
      "F1 score: 0.0578\n",
      "Kappa score: 0.5341\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.2845\n",
      "Precision: 0.0219\n",
      "Recall: 0.0773\n",
      "F1 score: 0.0334\n",
      "Kappa score: 0.5583\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.1304\n",
      "Precision: 0.0597\n",
      "Recall: 0.0749\n",
      "F1 score: 0.0354\n",
      "Kappa score: 0.2127\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.2696\n",
      "Precision: 0.0118\n",
      "Recall: 0.0476\n",
      "F1 score: 0.0187\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set8 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "385e0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_qwk = [logistic_regression_qwk_set1, logistic_regression_qwk_set2, logistic_regression_qwk_set3,\n",
    "                           logistic_regression_qwk_set4, logistic_regression_qwk_set5, logistic_regression_qwk_set6,\n",
    "                           logistic_regression_qwk_set7, logistic_regression_qwk_set8]\n",
    "random_forest_classifier_qwk = [random_forest_classifier_qwk_set1, random_forest_classifier_qwk_set2, random_forest_classifier_qwk_set3,\n",
    "                                random_forest_classifier_qwk_set4, random_forest_classifier_qwk_set5, random_forest_classifier_qwk_set6,\n",
    "                                random_forest_classifier_qwk_set7, random_forest_classifier_qwk_set8]\n",
    "adaboost_classifier_qwk = [adaboost_classifier_qwk_set1, adaboost_classifier_qwk_set2, adaboost_classifier_qwk_set3,\n",
    "                           adaboost_classifier_qwk_set4, adaboost_classifier_qwk_set5, adaboost_classifier_qwk_set6,\n",
    "                           adaboost_classifier_qwk_set7, adaboost_classifier_qwk_set8]\n",
    "k_neighbors_classifier_qwk = [k_neighbors_classifier_qwk_set1, k_neighbors_classifier_qwk_set2, k_neighbors_classifier_qwk_set3,\n",
    "                              k_neighbors_classifier_qwk_set4, k_neighbors_classifier_qwk_set5, k_neighbors_classifier_qwk_set6,\n",
    "                              k_neighbors_classifier_qwk_set7, k_neighbors_classifier_qwk_set8]\n",
    "support_vector_classifier_qwk = [support_vector_classifier_qwk_set1, support_vector_classifier_qwk_set2, support_vector_classifier_qwk_set3,\n",
    "                                 support_vector_classifier_qwk_set4, support_vector_classifier_qwk_set5, support_vector_classifier_qwk_set6,\n",
    "                                 support_vector_classifier_qwk_set7, support_vector_classifier_qwk_set8]\n",
    "\n",
    "metrics_list = [logistic_regression_qwk, random_forest_classifier_qwk, adaboost_classifier_qwk,\n",
    "               k_neighbors_classifier_qwk, support_vector_classifier_qwk]\n",
    "\n",
    "results_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "results_df.rename(columns = {0: 'Prompt-1', 1: 'Prompt-2', 2: 'Prompt-3', \n",
    "                            3: 'Prompt-4', 4: 'Prompt-5', 5: 'Prompt-6',\n",
    "                            6: 'Prompt-7', 7: 'Prompt-8'}, inplace = True)\n",
    "\n",
    "results_df.rename(index = {0: 'BERT + LR', 1: 'BERT + RF', \n",
    "                           2: 'BERT + Adaboost', 3: 'BERT + KNN', 4: 'BERT + SVC'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7055fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('Results/BERT + features.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
