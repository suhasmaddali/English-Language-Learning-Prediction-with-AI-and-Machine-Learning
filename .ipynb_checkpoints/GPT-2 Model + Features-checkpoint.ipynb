{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4f19d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from transformers import TFGPT2Model, GPT2Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "18d8f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "caa2f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any missing values from the data\n",
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8f4e8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay', 'rater1_domain1', 'rater2_domain1']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5d62ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the precision score between the true and predicted values\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calc_recall(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the recall score between the true and predicted values\n",
    "    \"\"\"\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calc_f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the f1-score between the true and predicted values\n",
    "    \"\"\"\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calc_cohen_kappa_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the cohen kappa score between the true and predicted values\n",
    "    \"\"\"\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calc_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy score between the true and predicted values\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dccb2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_function(y_actual, y_predictions, print_metrics = False):\n",
    "    \n",
    "    accuracy = calc_accuracy(y_actual, y_predictions)\n",
    "    precision = calc_precision(y_actual, y_predictions)\n",
    "    recall = calc_recall(y_actual, y_predictions)\n",
    "    f1 = calc_f1_score(y_actual, y_predictions)\n",
    "    kappa_score = calc_cohen_kappa_score(y_actual, y_predictions)\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1-Score:\", f1)\n",
    "        print(\"Cohen Kappa Score:\", kappa_score)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc48d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5fa2fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_classifiers(classifier_name = \"logistic_regression\"):\n",
    "    \"\"\"\n",
    "    Takes a regressor as input and returns a corresponding classifier object\n",
    "    \"\"\"\n",
    "    \n",
    "    if classifier_name == 'logistic_regression':\n",
    "        return LogisticRegression()\n",
    "    elif classifier_name == 'decision_tree_classifier':\n",
    "        return DecisionTreeClassifier()\n",
    "    elif classifier_name == 'random_forest_classifier':\n",
    "        return RandomForestClassifier()\n",
    "    elif classifier_name == 'gradient_boosting_classifier':\n",
    "        return GradientBoostingClassifier()\n",
    "    elif classifier_name == 'adaboost_classifier':\n",
    "        return AdaBoostClassifier()\n",
    "    elif classifier_name == 'k_neighbors_classifier':\n",
    "        return KNeighborsClassifier()\n",
    "    elif classifier_name == 'support_vector_classifier':\n",
    "        return SVC()\n",
    "    elif classifier_name == 'xgboost_classifier':\n",
    "        return XGBClassifier()\n",
    "    elif classifier_name == 'gaussian_naive_bayes_classifier':\n",
    "        return GaussianNB()\n",
    "    else:\n",
    "        raise ValueError(f\"Classifier {classifier_name} not supported for this problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "66c66657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrector(text):\n",
    "    spell_checker = SpellChecker()\n",
    "    correct_tokens = []\n",
    "    for token in tqdm(text.split()):\n",
    "        if spell_checker.correction(token.lower()):\n",
    "            correct_tokens.append(spell_checker.correction(token.lower()))\n",
    "        else:\n",
    "            correct_tokens.append(token.lower())\n",
    "    \n",
    "    return ' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "67bc841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 1]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25,\n",
    "                                                 random_state = 101, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "90e51c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1069, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d6eb5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 124439808\n"
     ]
    }
   ],
   "source": [
    "# This downloads the pre-trained weights from the huggingface website \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gpt_model = TFGPT2Model.from_pretrained('gpt2')\n",
    "print(f\"Total number of parameters: {gpt_model.count_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b1f50",
   "metadata": {},
   "source": [
    "### GPT-2 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bbd3d",
   "metadata": {},
   "source": [
    "#### Extracting GPT - 2 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6810143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:20<00:00,  3.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:07<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "28289efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4b48461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5f900a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783,)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3db16610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 10)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1334b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_function(input_data, output_data, ml_model = \"logistic_regression\",\n",
    "                             print_results = True, n_folds = 5):\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    kappa_score_list = []\n",
    "    \n",
    "    k_fold = KFold(n_splits=n_folds, shuffle=True, random_state=101)\n",
    "    X = pd.DataFrame(input_data.numpy()).copy()\n",
    "    y = output_data.copy()\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(k_fold.split(X, y)):\n",
    "        \n",
    "        train_embeddings, train_y = X.iloc[train_indices], y.iloc[train_indices]\n",
    "        val_embeddings, val_y = X.iloc[val_indices], y.iloc[val_indices]\n",
    "        model = choose_classifiers(ml_model)\n",
    "        model.fit(train_embeddings, train_y)\n",
    "        y_predictions = model.predict(val_embeddings)\n",
    "        accuracy_logistic_reg, precision_logistic_reg, recall_logistic_reg, f1_logistic_reg, kappa_score_logistic_reg = print_metrics_function(val_y, y_predictions)\n",
    "        accuracy_list.append(accuracy_logistic_reg)\n",
    "        precision_list.append(precision_logistic_reg)\n",
    "        recall_list.append(recall_logistic_reg)\n",
    "        f1_list.append(f1_logistic_reg)\n",
    "        kappa_score_list.append(kappa_score_logistic_reg)\n",
    "    \n",
    "    if print_results:\n",
    "        print(\"Average accuracy: {:.4f}\".format(np.mean(accuracy_list)))\n",
    "        print(\"Average precision: {:.4f}\".format(np.mean(precision_list)))\n",
    "        print(\"Average recall: {:.4f}\".format(np.mean(recall_list)))\n",
    "        print(\"Average F1 score: {:.4f}\".format(np.mean(f1_list)))\n",
    "        print(\"Average kappa score: {:.4f}\".format(np.mean(kappa_score_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e80057cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.4696\n",
      "Average precision: 0.2050\n",
      "Average recall: 0.2142\n",
      "Average F1 score: 0.1927\n",
      "Average kappa score: 0.6968\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.4705\n",
      "Average precision: 0.3792\n",
      "Average recall: 0.3936\n",
      "Average F1 score: 0.3590\n",
      "Average kappa score: 0.7572\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.4471\n",
      "Average precision: 0.1505\n",
      "Average recall: 0.2270\n",
      "Average F1 score: 0.1738\n",
      "Average kappa score: 0.5933\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.4256\n",
      "Average precision: 0.3591\n",
      "Average recall: 0.3596\n",
      "Average F1 score: 0.3459\n",
      "Average kappa score: 0.6927\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.3882\n",
      "Average precision: 0.0397\n",
      "Average recall: 0.1022\n",
      "Average F1 score: 0.0571\n",
      "Average kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb9b4f",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "00fd2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 2]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dfeccfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:37<00:00,  2.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "39c74db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7fdb9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.6562\n",
      "Average precision: 0.3667\n",
      "Average recall: 0.3418\n",
      "Average F1 score: 0.3473\n",
      "Average kappa score: 0.6111\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.6736\n",
      "Average precision: 0.4449\n",
      "Average recall: 0.3607\n",
      "Average F1 score: 0.3745\n",
      "Average kappa score: 0.6326\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.6208\n",
      "Average precision: 0.2453\n",
      "Average recall: 0.2734\n",
      "Average F1 score: 0.2567\n",
      "Average kappa score: 0.5046\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.6306\n",
      "Average precision: 0.4038\n",
      "Average recall: 0.3507\n",
      "Average F1 score: 0.3642\n",
      "Average kappa score: 0.5700\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.4097\n",
      "Average precision: 0.0709\n",
      "Average recall: 0.1733\n",
      "Average F1 score: 0.1006\n",
      "Average kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0111f",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e3ecd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 3]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0381ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [00:15<00:00,  5.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:02<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3fae3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6ff40830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.6594\n",
      "Average precision: 0.5287\n",
      "Average recall: 0.5086\n",
      "Average F1 score: 0.5094\n",
      "Average kappa score: 0.6634\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.6333\n",
      "Average precision: 0.5429\n",
      "Average recall: 0.5138\n",
      "Average F1 score: 0.5183\n",
      "Average kappa score: 0.6510\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.5290\n",
      "Average precision: 0.4431\n",
      "Average recall: 0.4698\n",
      "Average F1 score: 0.4363\n",
      "Average kappa score: 0.5467\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.6188\n",
      "Average precision: 0.5143\n",
      "Average recall: 0.4877\n",
      "Average F1 score: 0.4901\n",
      "Average kappa score: 0.6239\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.3870\n",
      "Average precision: 0.2959\n",
      "Average recall: 0.2550\n",
      "Average F1 score: 0.1482\n",
      "Average kappa score: 0.0164\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71087c46",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3dfbd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 4]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d8ddf703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 89/89 [00:15<00:00,  5.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "eb213cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "269f9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.6137\n",
      "Average precision: 0.6198\n",
      "Average recall: 0.5629\n",
      "Average F1 score: 0.5695\n",
      "Average kappa score: 0.6834\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.6052\n",
      "Average precision: 0.5982\n",
      "Average recall: 0.5668\n",
      "Average F1 score: 0.5698\n",
      "Average kappa score: 0.6964\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.5205\n",
      "Average precision: 0.5151\n",
      "Average recall: 0.5247\n",
      "Average F1 score: 0.5073\n",
      "Average kappa score: 0.6582\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.5473\n",
      "Average precision: 0.5330\n",
      "Average recall: 0.5112\n",
      "Average F1 score: 0.5139\n",
      "Average kappa score: 0.6487\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.3609\n",
      "Average precision: 0.0902\n",
      "Average recall: 0.2500\n",
      "Average F1 score: 0.1323\n",
      "Average kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4e70e",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4bae6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 5]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a1b79613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:18<00:00,  5.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  7.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "65ea9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "863b3596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.6703\n",
      "Average precision: 0.5622\n",
      "Average recall: 0.5404\n",
      "Average F1 score: 0.5476\n",
      "Average kappa score: 0.7884\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.6510\n",
      "Average precision: 0.5282\n",
      "Average recall: 0.5232\n",
      "Average F1 score: 0.5231\n",
      "Average kappa score: 0.7772\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.5312\n",
      "Average precision: 0.2404\n",
      "Average recall: 0.3171\n",
      "Average F1 score: 0.2581\n",
      "Average kappa score: 0.5674\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.6281\n",
      "Average precision: 0.5518\n",
      "Average recall: 0.5385\n",
      "Average F1 score: 0.5404\n",
      "Average kappa score: 0.7662\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.3656\n",
      "Average precision: 0.1986\n",
      "Average recall: 0.2045\n",
      "Average F1 score: 0.1171\n",
      "Average kappa score: 0.0895\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce85418",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3267402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 6]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "da3cc5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:20<00:00,  4.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:04<00:00,  4.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d4d21d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cf856cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.6056\n",
      "Average precision: 0.5039\n",
      "Average recall: 0.4389\n",
      "Average F1 score: 0.4565\n",
      "Average kappa score: 0.6614\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.5951\n",
      "Average precision: 0.5284\n",
      "Average recall: 0.4609\n",
      "Average F1 score: 0.4709\n",
      "Average kappa score: 0.6657\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.4618\n",
      "Average precision: 0.3434\n",
      "Average recall: 0.3777\n",
      "Average F1 score: 0.3289\n",
      "Average kappa score: 0.5657\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.5604\n",
      "Average precision: 0.4743\n",
      "Average recall: 0.4289\n",
      "Average F1 score: 0.4392\n",
      "Average kappa score: 0.6340\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.4472\n",
      "Average precision: 0.0894\n",
      "Average recall: 0.2000\n",
      "Average F1 score: 0.1234\n",
      "Average kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf78695",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c4277b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 7]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "64d55069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:22<00:00,  3.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "eace1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "17f10447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.1689\n",
      "Average precision: 0.1075\n",
      "Average recall: 0.1170\n",
      "Average F1 score: 0.0847\n",
      "Average kappa score: 0.6347\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.1482\n",
      "Average precision: 0.1154\n",
      "Average recall: 0.1104\n",
      "Average F1 score: 0.0927\n",
      "Average kappa score: 0.6324\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.1578\n",
      "Average precision: 0.0217\n",
      "Average recall: 0.0793\n",
      "Average F1 score: 0.0301\n",
      "Average kappa score: 0.3886\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.1028\n",
      "Average precision: 0.0947\n",
      "Average recall: 0.0933\n",
      "Average F1 score: 0.0866\n",
      "Average kappa score: 0.5403\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.1355\n",
      "Average precision: 0.0067\n",
      "Average recall: 0.0496\n",
      "Average F1 score: 0.0118\n",
      "Average kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff6893",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "27d74039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 8]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fe490151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:16<00:00,  2.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1c1b433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f98ba9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Average accuracy: 0.2110\n",
      "Average precision: 0.0409\n",
      "Average recall: 0.0594\n",
      "Average F1 score: 0.0416\n",
      "Average kappa score: 0.4955\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Average accuracy: 0.2110\n",
      "Average precision: 0.0315\n",
      "Average recall: 0.0571\n",
      "Average F1 score: 0.0357\n",
      "Average kappa score: 0.4692\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Average accuracy: 0.2317\n",
      "Average precision: 0.0170\n",
      "Average recall: 0.0584\n",
      "Average F1 score: 0.0261\n",
      "Average kappa score: 0.4265\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Average accuracy: 0.1575\n",
      "Average precision: 0.0624\n",
      "Average recall: 0.0678\n",
      "Average F1 score: 0.0579\n",
      "Average kappa score: 0.4103\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Average accuracy: 0.2283\n",
      "Average precision: 0.0096\n",
      "Average recall: 0.0417\n",
      "Average F1 score: 0.0155\n",
      "Average kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
