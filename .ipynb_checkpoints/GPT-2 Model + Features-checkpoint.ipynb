{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4f19d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from transformers import TFGPT2Model, GPT2Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18d8f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "caa2f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any missing values from the data\n",
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8f4e8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay', 'rater1_domain1', 'rater2_domain1']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5d62ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the precision score between the true and predicted values\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calc_recall(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the recall score between the true and predicted values\n",
    "    \"\"\"\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calc_f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the f1-score between the true and predicted values\n",
    "    \"\"\"\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calc_cohen_kappa_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the cohen kappa score between the true and predicted values\n",
    "    \"\"\"\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calc_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy score between the true and predicted values\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dccb2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_function(y_actual, y_predictions, print_metrics = False):\n",
    "    \n",
    "    accuracy = calc_accuracy(y_actual, y_predictions)\n",
    "    precision = calc_precision(y_actual, y_predictions)\n",
    "    recall = calc_recall(y_actual, y_predictions)\n",
    "    f1 = calc_f1_score(y_actual, y_predictions)\n",
    "    kappa_score = calc_cohen_kappa_score(y_actual, y_predictions)\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1-Score:\", f1)\n",
    "        print(\"Cohen Kappa Score:\", kappa_score)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc48d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5fa2fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_classifiers(classifier_name = \"logistic_regression\"):\n",
    "    \"\"\"\n",
    "    Takes a regressor as input and returns a corresponding classifier object\n",
    "    \"\"\"\n",
    "    \n",
    "    if classifier_name == 'logistic_regression':\n",
    "        return LogisticRegression()\n",
    "    elif classifier_name == 'decision_tree_classifier':\n",
    "        return DecisionTreeClassifier()\n",
    "    elif classifier_name == 'random_forest_classifier':\n",
    "        return RandomForestClassifier()\n",
    "    elif classifier_name == 'gradient_boosting_classifier':\n",
    "        return GradientBoostingClassifier()\n",
    "    elif classifier_name == 'adaboost_classifier':\n",
    "        return AdaBoostClassifier()\n",
    "    elif classifier_name == 'k_neighbors_classifier':\n",
    "        return KNeighborsClassifier()\n",
    "    elif classifier_name == 'support_vector_classifier':\n",
    "        return SVC()\n",
    "    elif classifier_name == 'xgboost_classifier':\n",
    "        return XGBClassifier()\n",
    "    elif classifier_name == 'gaussian_naive_bayes_classifier':\n",
    "        return GaussianNB()\n",
    "    else:\n",
    "        raise ValueError(f\"Classifier {classifier_name} not supported for this problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "66c66657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrector(text):\n",
    "    spell_checker = SpellChecker()\n",
    "    correct_tokens = []\n",
    "    for token in tqdm(text.split()):\n",
    "        if spell_checker.correction(token.lower()):\n",
    "            correct_tokens.append(spell_checker.correction(token.lower()))\n",
    "        else:\n",
    "            correct_tokens.append(token.lower())\n",
    "    \n",
    "    return ' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67bc841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 1]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25,\n",
    "                                                 random_state = 101, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d6eb5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 124439808\n"
     ]
    }
   ],
   "source": [
    "# This downloads the pre-trained weights from the huggingface website \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gpt_model = TFGPT2Model.from_pretrained('gpt2')\n",
    "print(f\"Total number of parameters: {gpt_model.count_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b1f50",
   "metadata": {},
   "source": [
    "### GPT-2 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bbd3d",
   "metadata": {},
   "source": [
    "#### Extracting GPT - 2 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6810143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:20<00:00,  3.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:07<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "28289efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1334b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_function(input_data, output_data, ml_model = \"logistic_regression\",\n",
    "                             print_results = True, n_folds = 5, return_kappa_scores = True):\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    kappa_score_list = []\n",
    "    \n",
    "    k_fold = KFold(n_splits=n_folds, shuffle=True, random_state=101)\n",
    "    X = pd.DataFrame(input_data.numpy()).copy()\n",
    "    y = output_data.copy()\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(k_fold.split(X, y)):\n",
    "        \n",
    "        train_embeddings, train_y = X.iloc[train_indices], y.iloc[train_indices]\n",
    "        val_embeddings, val_y = X.iloc[val_indices], y.iloc[val_indices]\n",
    "        model = choose_classifiers(ml_model)\n",
    "        model.fit(train_embeddings, train_y)\n",
    "        y_predictions = model.predict(val_embeddings)\n",
    "        accuracy_logistic_reg, precision_logistic_reg, recall_logistic_reg, f1_logistic_reg, kappa_score_logistic_reg = print_metrics_function(val_y, y_predictions)\n",
    "        accuracy_list.append(accuracy_logistic_reg)\n",
    "        precision_list.append(precision_logistic_reg)\n",
    "        recall_list.append(recall_logistic_reg)\n",
    "        f1_list.append(f1_logistic_reg)\n",
    "        kappa_score_list.append(kappa_score_logistic_reg)\n",
    "    \n",
    "    if print_results:\n",
    "        \n",
    "        print(\"Accuracy: {:.4f}\".format(np.max(accuracy_list)))\n",
    "        print(\"Precision: {:.4f}\".format(np.max(precision_list)))\n",
    "        print(\"Recall: {:.4f}\".format(np.max(recall_list)))\n",
    "        print(\"F1 score: {:.4f}\".format(np.max(f1_list)))\n",
    "        print(\"Kappa score: {:.4f}\".format(np.max(kappa_score_list)))\n",
    "        \n",
    "    if return_kappa_scores:\n",
    "        \n",
    "        return round(np.max(kappa_score_list), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e80057cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5140\n",
      "Precision: 0.2333\n",
      "Recall: 0.2635\n",
      "F1 score: 0.2251\n",
      "Kappa score: 0.7573\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.5047\n",
      "Precision: 0.4617\n",
      "Recall: 0.4366\n",
      "F1 score: 0.3916\n",
      "Kappa score: 0.7951\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.4860\n",
      "Precision: 0.1966\n",
      "Recall: 0.2739\n",
      "F1 score: 0.2219\n",
      "Kappa score: 0.6484\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.4134\n",
      "Recall: 0.4299\n",
      "F1 score: 0.4012\n",
      "Kappa score: 0.7475\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4085\n",
      "Precision: 0.0421\n",
      "Recall: 0.1111\n",
      "F1 score: 0.0610\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set1 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb9b4f",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "00fd2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 2]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dfeccfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:36<00:00,  2.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "39c74db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7fdb9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6944\n",
      "Precision: 0.4026\n",
      "Recall: 0.3707\n",
      "F1 score: 0.3760\n",
      "Kappa score: 0.6224\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.7118\n",
      "Precision: 0.6938\n",
      "Recall: 0.4117\n",
      "F1 score: 0.4396\n",
      "Kappa score: 0.6876\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.6493\n",
      "Precision: 0.2840\n",
      "Recall: 0.3228\n",
      "F1 score: 0.2994\n",
      "Kappa score: 0.6054\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.6701\n",
      "Precision: 0.4654\n",
      "Recall: 0.3809\n",
      "F1 score: 0.3860\n",
      "Kappa score: 0.6209\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4340\n",
      "Precision: 0.0792\n",
      "Recall: 0.2000\n",
      "F1 score: 0.1134\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set2 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0111f",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e3ecd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 3]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0381ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [00:15<00:00,  5.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:02<00:00,  8.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3fae3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6ff40830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6848\n",
      "Precision: 0.6070\n",
      "Recall: 0.5214\n",
      "F1 score: 0.5382\n",
      "Kappa score: 0.6977\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6630\n",
      "Precision: 0.6198\n",
      "Recall: 0.5455\n",
      "F1 score: 0.5564\n",
      "Kappa score: 0.6745\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5833\n",
      "Precision: 0.4836\n",
      "Recall: 0.5385\n",
      "F1 score: 0.4862\n",
      "Kappa score: 0.5782\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.6486\n",
      "Precision: 0.5897\n",
      "Recall: 0.5064\n",
      "F1 score: 0.5199\n",
      "Kappa score: 0.6466\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4312\n",
      "Precision: 0.3573\n",
      "Recall: 0.2603\n",
      "F1 score: 0.1571\n",
      "Kappa score: 0.0336\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set3 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71087c46",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3dfbd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 4]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d8ddf703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 89/89 [00:15<00:00,  5.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  7.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eb213cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "269f9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6572\n",
      "Precision: 0.6713\n",
      "Recall: 0.6318\n",
      "F1 score: 0.6370\n",
      "Kappa score: 0.7417\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6254\n",
      "Precision: 0.6438\n",
      "Recall: 0.5879\n",
      "F1 score: 0.5898\n",
      "Kappa score: 0.7036\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5493\n",
      "Precision: 0.5553\n",
      "Recall: 0.5844\n",
      "F1 score: 0.5504\n",
      "Kappa score: 0.6842\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.5669\n",
      "Precision: 0.5700\n",
      "Recall: 0.5546\n",
      "F1 score: 0.5578\n",
      "Kappa score: 0.6954\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4205\n",
      "Precision: 0.1051\n",
      "Recall: 0.2500\n",
      "F1 score: 0.1480\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set4 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4e70e",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4bae6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 5]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a1b79613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:17<00:00,  5.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "65ea9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "863b3596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6990\n",
      "Precision: 0.6426\n",
      "Recall: 0.6222\n",
      "F1 score: 0.6304\n",
      "Kappa score: 0.8350\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6851\n",
      "Precision: 0.7477\n",
      "Recall: 0.6761\n",
      "F1 score: 0.7037\n",
      "Kappa score: 0.8315\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5744\n",
      "Precision: 0.3123\n",
      "Recall: 0.3272\n",
      "F1 score: 0.2694\n",
      "Kappa score: 0.5911\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.6540\n",
      "Precision: 0.6412\n",
      "Recall: 0.6324\n",
      "F1 score: 0.6361\n",
      "Kappa score: 0.7906\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.3979\n",
      "Precision: 0.2948\n",
      "Recall: 0.2103\n",
      "F1 score: 0.1334\n",
      "Kappa score: 0.1239\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set5 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce85418",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3267402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 6]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "da3cc5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:20<00:00,  4.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:04<00:00,  5.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d4d21d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cf856cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.5354\n",
      "Recall: 0.4534\n",
      "F1 score: 0.4719\n",
      "Kappa score: 0.7085\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6424\n",
      "Precision: 0.6293\n",
      "Recall: 0.5438\n",
      "F1 score: 0.5323\n",
      "Kappa score: 0.6943\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5139\n",
      "Precision: 0.4095\n",
      "Recall: 0.4101\n",
      "F1 score: 0.3605\n",
      "Kappa score: 0.6143\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.6354\n",
      "Precision: 0.5274\n",
      "Recall: 0.4898\n",
      "F1 score: 0.4942\n",
      "Kappa score: 0.6747\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5208\n",
      "Precision: 0.1042\n",
      "Recall: 0.2000\n",
      "F1 score: 0.1370\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set6 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf78695",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c4277b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 7]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "64d55069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:21<00:00,  3.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eace1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "17f10447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.1992\n",
      "Precision: 0.1424\n",
      "Recall: 0.1491\n",
      "F1 score: 0.0989\n",
      "Kappa score: 0.6746\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.1673\n",
      "Precision: 0.1614\n",
      "Recall: 0.1547\n",
      "F1 score: 0.1215\n",
      "Kappa score: 0.6851\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.1793\n",
      "Precision: 0.0284\n",
      "Recall: 0.1103\n",
      "F1 score: 0.0366\n",
      "Kappa score: 0.5554\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.1116\n",
      "Precision: 0.1061\n",
      "Recall: 0.1170\n",
      "F1 score: 0.1034\n",
      "Kappa score: 0.5814\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.1554\n",
      "Precision: 0.0078\n",
      "Recall: 0.0526\n",
      "F1 score: 0.0134\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set7 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff6893",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "27d74039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 8]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fe490151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:15<00:00,  2.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1c1b433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f98ba9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.2414\n",
      "Precision: 0.0775\n",
      "Recall: 0.0762\n",
      "F1 score: 0.0650\n",
      "Kappa score: 0.5645\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.2155\n",
      "Precision: 0.0637\n",
      "Recall: 0.0986\n",
      "F1 score: 0.0729\n",
      "Kappa score: 0.4796\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.2586\n",
      "Precision: 0.0226\n",
      "Recall: 0.0846\n",
      "F1 score: 0.0357\n",
      "Kappa score: 0.5801\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.1913\n",
      "Precision: 0.0994\n",
      "Recall: 0.1193\n",
      "F1 score: 0.1014\n",
      "Kappa score: 0.5126\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.2696\n",
      "Precision: 0.0118\n",
      "Recall: 0.0476\n",
      "F1 score: 0.0187\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set8 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cfded9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_qwk = [logistic_regression_qwk_set1, logistic_regression_qwk_set2, logistic_regression_qwk_set3,\n",
    "                           logistic_regression_qwk_set4, logistic_regression_qwk_set5, logistic_regression_qwk_set6,\n",
    "                           logistic_regression_qwk_set7, logistic_regression_qwk_set8]\n",
    "random_forest_classifier_qwk = [random_forest_classifier_qwk_set1, random_forest_classifier_qwk_set2, random_forest_classifier_qwk_set3,\n",
    "                                random_forest_classifier_qwk_set4, random_forest_classifier_qwk_set5, random_forest_classifier_qwk_set6,\n",
    "                                random_forest_classifier_qwk_set7, random_forest_classifier_qwk_set8]\n",
    "adaboost_classifier_qwk = [adaboost_classifier_qwk_set1, adaboost_classifier_qwk_set2, adaboost_classifier_qwk_set3,\n",
    "                           adaboost_classifier_qwk_set4, adaboost_classifier_qwk_set5, adaboost_classifier_qwk_set6,\n",
    "                           adaboost_classifier_qwk_set7, adaboost_classifier_qwk_set8]\n",
    "k_neighbors_classifier_qwk = [k_neighbors_classifier_qwk_set1, k_neighbors_classifier_qwk_set2, k_neighbors_classifier_qwk_set3,\n",
    "                              k_neighbors_classifier_qwk_set4, k_neighbors_classifier_qwk_set5, k_neighbors_classifier_qwk_set6,\n",
    "                              k_neighbors_classifier_qwk_set7, k_neighbors_classifier_qwk_set8]\n",
    "support_vector_classifier_qwk = [support_vector_classifier_qwk_set1, support_vector_classifier_qwk_set2, support_vector_classifier_qwk_set3,\n",
    "                                 support_vector_classifier_qwk_set4, support_vector_classifier_qwk_set5, support_vector_classifier_qwk_set6,\n",
    "                                 support_vector_classifier_qwk_set7, support_vector_classifier_qwk_set8]\n",
    "\n",
    "metrics_list = [logistic_regression_qwk, random_forest_classifier_qwk, adaboost_classifier_qwk,\n",
    "               k_neighbors_classifier_qwk, support_vector_classifier_qwk]\n",
    "\n",
    "results_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "results_df.rename(columns = {0: 'Prompt-1', 1: 'Prompt-2', 2: 'Prompt-3', \n",
    "                            3: 'Prompt-4', 4: 'Prompt-5', 5: 'Prompt-6',\n",
    "                            6: 'Prompt-7', 7: 'Prompt-8'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fea59996",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_qwk = [logistic_regression_qwk_set1, logistic_regression_qwk_set2, logistic_regression_qwk_set3,                           logistic_regression_qwk_set4, logistic_regression_qwk_set5, logistic_regression_qwk_set6,                           logistic_regression_qwk_set7, logistic_regression_qwk_set8]\n",
    "random_forest_classifier_qwk = [random_forest_classifier_qwk_set1, random_forest_classifier_qwk_set2, random_forest_classifier_qwk_set3,                                random_forest_classifier_qwk_set4, random_forest_classifier_qwk_set5, random_forest_classifier_qwk_set6,                                random_forest_classifier_qwk_set7, random_forest_classifier_qwk_set8]\n",
    "adaboost_classifier_qwk = [adaboost_classifier_qwk_set1, adaboost_classifier_qwk_set2, adaboost_classifier_qwk_set3,                           adaboost_classifier_qwk_set4, adaboost_classifier_qwk_set5, adaboost_classifier_qwk_set6,                           adaboost_classifier_qwk_set7, adaboost_classifier_qwk_set8]\n",
    "k_neighbors_classifier_qwk = [k_neighbors_classifier_qwk_set1, k_neighbors_classifier_qwk_set2, k_neighbors_classifier_qwk_set3,                              k_neighbors_classifier_qwk_set4, k_neighbors_classifier_qwk_set5, k_neighbors_classifier_qwk_set6,                              k_neighbors_classifier_qwk_set7, k_neighbors_classifier_qwk_set8]\n",
    "support_vector_classifier_qwk = [support_vector_classifier_qwk_set1, support_vector_classifier_qwk_set2, support_vector_classifier_qwk_set3,                                 support_vector_classifier_qwk_set4, support_vector_classifier_qwk_set5, support_vector_classifier_qwk_set6,                                 support_vector_classifier_qwk_set7, support_vector_classifier_qwk_set8]\n",
    "\n",
    "metrics_list = [logistic_regression_qwk, random_forest_classifier_qwk, adaboost_classifier_qwk,               k_neighbors_classifier_qwk, support_vector_classifier_qwk]\n",
    "\n",
    "results_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "results_df.rename(columns = {0: 'Prompt-1', 1: 'Prompt-2', 2: 'Prompt-3', \n",
    "                            3: 'Prompt-4', 4: 'Prompt-5', 5: 'Prompt-6',\n",
    "                            6: 'Prompt-7', 7: 'Prompt-8'}, inplace = True)\n",
    "\n",
    "results_df.rename(index = {0: 'GPT-2 + LR', 1: 'GPT-2 + RF', \n",
    "                           2: 'GPT-2 + Adaboost', 3: 'GPT-2 + KNN', 4: 'GPT-2 + SVC'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "40f4dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('Results/gpt-2 + features.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
