{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f19d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from transformers import TFGPT2Model, GPT2Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18d8f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caa2f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing any missing values from the data\n",
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f4e8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay', 'rater1_domain1', 'rater2_domain1']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d62ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(y_true, y_pred, average='macro'):\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(y_true, y_pred, average='macro'):\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred, average='macro'):\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calculate_cohen_kappa_score(y_true, y_pred):\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def print_metrics_function(y_actual, y_predictions):\n",
    "    \n",
    "    accuracy = calculate_accuracy(y_actual, y_predictions)\n",
    "    precision = calculate_precision(y_actual, y_predictions)\n",
    "    recall = calculate_recall(y_actual, y_predictions)\n",
    "    f1 = calculate_f1_score(y_actual, y_predictions)\n",
    "    kappa_score = calculate_cohen_kappa_score(y_actual, y_predictions)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc48d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fa2fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_classifiers(classifier_name = \"logistic_regression\"):\n",
    "    \n",
    "    if classifier_name == 'logistic_regression':\n",
    "        return LogisticRegression()\n",
    "    elif classifier_name == 'random_forest_classifier':\n",
    "        return RandomForestClassifier()\n",
    "    elif classifier_name == 'gradient_boosting_classifier':\n",
    "        return GradientBoostingClassifier()\n",
    "    elif classifier_name == 'adaboost_classifier':\n",
    "        return AdaBoostClassifier()\n",
    "    elif classifier_name == 'k_neighbors_classifier':\n",
    "        return KNeighborsClassifier()\n",
    "    elif classifier_name == 'support_vector_classifier':\n",
    "        return SVC()\n",
    "    else:\n",
    "        raise ValueError(f\"Classifier {classifier_name} not supported for this problem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67bc841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 1]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25,\n",
    "                                                 random_state = 101, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6eb5038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 124439808\n"
     ]
    }
   ],
   "source": [
    "# This downloads the pre-trained weights from the huggingface website \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gpt_model = TFGPT2Model.from_pretrained('gpt2')\n",
    "print(f\"Total number of parameters: {gpt_model.count_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b1f50",
   "metadata": {},
   "source": [
    "### GPT-2 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bbd3d",
   "metadata": {},
   "source": [
    "#### Extracting GPT - 2 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6810143d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 67/67 [00:20<00:00,  3.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:07<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37394f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1069, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6e7454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "with open('Models/bow_vectorizer1.pkl', 'wb') as f:\n",
    "    pickle.dump(bow_vectorizer, f)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "with open('Models/tfidf_vectorizer1.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "with open('Models/scaler1.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "with open('Models/pca1.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_bow, f)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "with open('Models/pca2.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_tfidf, f)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "with open('Models/scaler2.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1334b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_function(input_data, output_data, ml_model = \"logistic_regression\",\n",
    "                             print_results = True, n_folds = 5, return_kappa_scores = True,\n",
    "                             save_model = False):\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    kappa_score_list = []\n",
    "    \n",
    "    k_fold = KFold(n_splits=n_folds, shuffle=True, random_state=101)\n",
    "    X = pd.DataFrame(input_data.numpy()).copy()\n",
    "    y = output_data.copy()\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(k_fold.split(X, y)):\n",
    "        \n",
    "        train_embeddings, train_y = X.iloc[train_indices], y.iloc[train_indices]\n",
    "        val_embeddings, val_y = X.iloc[val_indices], y.iloc[val_indices]\n",
    "        model = choose_classifiers(ml_model)\n",
    "        model.fit(train_embeddings, train_y)\n",
    "        y_predictions = model.predict(val_embeddings)\n",
    "        \n",
    "        if save_model:\n",
    "            with open('Models/best_model.pkl', 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "               \n",
    "        accuracy_logistic_reg, precision_logistic_reg, recall_logistic_reg, f1_logistic_reg, kappa_score_logistic_reg = print_metrics_function(val_y, y_predictions)\n",
    "        accuracy_list.append(accuracy_logistic_reg)\n",
    "        precision_list.append(precision_logistic_reg)\n",
    "        recall_list.append(recall_logistic_reg)\n",
    "        f1_list.append(f1_logistic_reg)\n",
    "        kappa_score_list.append(kappa_score_logistic_reg)\n",
    "    \n",
    "    if print_results:\n",
    "        \n",
    "        print(\"Accuracy: {:.4f}\".format(np.max(accuracy_list)))\n",
    "        print(\"Precision: {:.4f}\".format(np.max(precision_list)))\n",
    "        print(\"Recall: {:.4f}\".format(np.max(recall_list)))\n",
    "        print(\"F1 score: {:.4f}\".format(np.max(f1_list)))\n",
    "        print(\"Kappa score: {:.4f}\".format(np.max(kappa_score_list)))\n",
    "        \n",
    "    if return_kappa_scores:\n",
    "        \n",
    "        return round(np.max(kappa_score_list), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e80057cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.4112\n",
      "Precision: 0.2136\n",
      "Recall: 0.3158\n",
      "F1 score: 0.2353\n",
      "Kappa score: 0.6461\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.4907\n",
      "Precision: 0.4199\n",
      "Recall: 0.4151\n",
      "F1 score: 0.4103\n",
      "Kappa score: 0.7968\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.4860\n",
      "Precision: 0.1966\n",
      "Recall: 0.2739\n",
      "F1 score: 0.2219\n",
      "Kappa score: 0.6484\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.2394\n",
      "Precision: 0.1451\n",
      "Recall: 0.2248\n",
      "F1 score: 0.0956\n",
      "Kappa score: 0.2855\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4085\n",
      "Precision: 0.1387\n",
      "Recall: 0.1159\n",
      "F1 score: 0.0721\n",
      "Kappa score: 0.0905\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set1 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\",\n",
    "                                                             save_model = True)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set1 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb9b4f",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "00fd2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 2]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "dfeccfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:40<00:00,  2.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:09<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "39c74db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7fdb9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.6250\n",
      "Precision: 0.4814\n",
      "Recall: 0.3757\n",
      "F1 score: 0.3934\n",
      "Kappa score: 0.5783\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6910\n",
      "Precision: 0.4745\n",
      "Recall: 0.3742\n",
      "F1 score: 0.3928\n",
      "Kappa score: 0.6480\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.6493\n",
      "Precision: 0.3364\n",
      "Recall: 0.3970\n",
      "F1 score: 0.3631\n",
      "Kappa score: 0.6054\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.4167\n",
      "Precision: 0.2728\n",
      "Recall: 0.3191\n",
      "F1 score: 0.2417\n",
      "Kappa score: 0.3079\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5799\n",
      "Precision: 0.2170\n",
      "Recall: 0.2512\n",
      "F1 score: 0.2250\n",
      "Kappa score: 0.3853\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set2 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set2 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0111f",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e3ecd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 3]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0381ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [00:17<00:00,  4.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:03<00:00,  6.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3fae3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6ff40830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5399\n",
      "Precision: 0.4744\n",
      "Recall: 0.4721\n",
      "F1 score: 0.4705\n",
      "Kappa score: 0.4953\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.7029\n",
      "Precision: 0.6196\n",
      "Recall: 0.5447\n",
      "F1 score: 0.5421\n",
      "Kappa score: 0.7073\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5507\n",
      "Precision: 0.6314\n",
      "Recall: 0.4779\n",
      "F1 score: 0.4922\n",
      "Kappa score: 0.6039\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.3949\n",
      "Precision: 0.2408\n",
      "Recall: 0.3234\n",
      "F1 score: 0.2041\n",
      "Kappa score: 0.1188\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5616\n",
      "Precision: 0.4573\n",
      "Recall: 0.4140\n",
      "F1 score: 0.4162\n",
      "Kappa score: 0.4922\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set3 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set3 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71087c46",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3dfbd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 4]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d8ddf703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 89/89 [00:17<00:00,  4.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "eb213cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "269f9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5972\n",
      "Precision: 0.6008\n",
      "Recall: 0.6012\n",
      "F1 score: 0.5783\n",
      "Kappa score: 0.7003\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6572\n",
      "Precision: 0.6479\n",
      "Recall: 0.6295\n",
      "F1 score: 0.6342\n",
      "Kappa score: 0.7457\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5760\n",
      "Precision: 0.5663\n",
      "Recall: 0.5603\n",
      "F1 score: 0.5541\n",
      "Kappa score: 0.7001\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.4170\n",
      "Precision: 0.3477\n",
      "Recall: 0.3330\n",
      "F1 score: 0.2725\n",
      "Kappa score: 0.2306\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4735\n",
      "Precision: 0.4756\n",
      "Recall: 0.3419\n",
      "F1 score: 0.2696\n",
      "Kappa score: 0.4827\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set4 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set4 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4e70e",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4bae6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 5]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a1b79613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:19<00:00,  4.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:03<00:00,  6.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "65ea9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "863b3596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5590\n",
      "Precision: 0.4658\n",
      "Recall: 0.4901\n",
      "F1 score: 0.4690\n",
      "Kappa score: 0.7208\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6713\n",
      "Precision: 0.5542\n",
      "Recall: 0.5499\n",
      "F1 score: 0.5419\n",
      "Kappa score: 0.8142\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5779\n",
      "Precision: 0.4298\n",
      "Recall: 0.3872\n",
      "F1 score: 0.3544\n",
      "Kappa score: 0.6635\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.3633\n",
      "Precision: 0.4282\n",
      "Recall: 0.2950\n",
      "F1 score: 0.2341\n",
      "Kappa score: 0.3841\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.4325\n",
      "Precision: 0.3776\n",
      "Recall: 0.2464\n",
      "F1 score: 0.1940\n",
      "Kappa score: 0.3547\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set5 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set5 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce85418",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3267402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 6]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "da3cc5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:22<00:00,  3.96it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [00:05<00:00,  4.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d4d21d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "cf856cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.5417\n",
      "Precision: 0.4840\n",
      "Recall: 0.4609\n",
      "F1 score: 0.4703\n",
      "Kappa score: 0.7093\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.6632\n",
      "Precision: 0.6059\n",
      "Recall: 0.5034\n",
      "F1 score: 0.5137\n",
      "Kappa score: 0.7029\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.5799\n",
      "Precision: 0.3583\n",
      "Recall: 0.3694\n",
      "F1 score: 0.3398\n",
      "Kappa score: 0.6089\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.3646\n",
      "Precision: 0.4364\n",
      "Recall: 0.4467\n",
      "F1 score: 0.3115\n",
      "Kappa score: 0.5422\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.5208\n",
      "Precision: 0.1042\n",
      "Recall: 0.2000\n",
      "F1 score: 0.1370\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set6 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set6 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf78695",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c4277b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 7]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "64d55069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:23<00:00,  3.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "eace1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "17f10447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.1155\n",
      "Precision: 0.1001\n",
      "Recall: 0.0923\n",
      "F1 score: 0.0813\n",
      "Kappa score: 0.5224\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.1952\n",
      "Precision: 0.2008\n",
      "Recall: 0.1528\n",
      "F1 score: 0.1433\n",
      "Kappa score: 0.6623\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.1833\n",
      "Precision: 0.0284\n",
      "Recall: 0.1103\n",
      "F1 score: 0.0366\n",
      "Kappa score: 0.5554\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.0478\n",
      "Precision: 0.0682\n",
      "Recall: 0.0757\n",
      "F1 score: 0.0277\n",
      "Kappa score: 0.1870\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.1633\n",
      "Precision: 0.0384\n",
      "Recall: 0.0769\n",
      "F1 score: 0.0373\n",
      "Kappa score: 0.1454\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set7 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set7 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff6893",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "27d74039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 8]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "fe490151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:16<00:00,  2.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code can take about 5 - 10 minutes to run depending on the speed of the system\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "train_encodings = tokenizer(list(X_train['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).batch(BATCH_SIZE)\n",
    "\n",
    "test_encodings = tokenizer(list(X_test['preprocessed_text']), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='tf')\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "embeddings_train = []\n",
    "for batch in tqdm(train_dataset):\n",
    "    embeddings_train.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_train = tf.concat(embeddings_train, axis=0)\n",
    "\n",
    "embeddings_test = []\n",
    "for batch in tqdm(test_dataset):\n",
    "    embeddings_test.append(gpt_model(batch[0]['input_ids'])[0][:, -1, :])\n",
    "embeddings_test = tf.concat(embeddings_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1c1b433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_bow = bow_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['preprocessed_text']).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['preprocessed_text']).toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_bow = scaler.fit_transform(X_train_bow)\n",
    "X_test_bow = scaler.transform(X_test_bow)\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "\n",
    "pca_bow = PCA()\n",
    "X_train_bow_pca = pca_bow.fit_transform(X_train_bow)\n",
    "X_test_bow_pca = pca_bow.transform(X_test_bow)\n",
    "\n",
    "variance_ratio_bow = np.cumsum(pca_bow.explained_variance_ratio_)\n",
    "n_components_bow = np.argmax(variance_ratio_bow >= 0.95) + 1\n",
    "X_train_bow_pca = X_train_bow_pca[:, :n_components_bow]\n",
    "X_test_bow_pca = X_test_bow_pca[:, :n_components_bow]\n",
    "\n",
    "pca_tfidf = PCA()\n",
    "X_train_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_pca = pca_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "variance_ratio_tfidf = np.cumsum(pca_tfidf.explained_variance_ratio_)\n",
    "n_components_tfidf = np.argmax(variance_ratio_tfidf >= 0.95) + 1\n",
    "X_train_tfidf_pca = X_train_tfidf_pca[:, :n_components_tfidf]\n",
    "X_test_tfidf_pca = X_test_tfidf_pca[:, :n_components_tfidf]\n",
    "\n",
    "X_train_bow = tf.convert_to_tensor(X_train_bow_pca, dtype = tf.float32)\n",
    "X_test_bow = tf.convert_to_tensor(X_test_bow_pca, dtype = tf.float32)\n",
    "\n",
    "X_train_tfidf = tf.convert_to_tensor(X_train_tfidf_pca, dtype = tf.float32)\n",
    "X_test_tfidf = tf.convert_to_tensor(X_test_tfidf_pca, dtype = tf.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop(['preprocessed_text'], axis = 1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(['preprocessed_text'], axis = 1))\n",
    "X_train_features = tf.constant(X_train_scaled.astype('float32'))\n",
    "X_test_features = tf.constant(X_test_scaled.astype('float32'))\n",
    "\n",
    "embeddings_train = tf.concat([embeddings_train, X_train_features, X_train_bow], axis = 1)\n",
    "embeddings_test = tf.concat([embeddings_test, X_test_features, X_test_bow], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f98ba9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Logistic Regression-----------------------\n",
      "Accuracy: 0.2155\n",
      "Precision: 0.0813\n",
      "Recall: 0.0988\n",
      "F1 score: 0.0853\n",
      "Kappa score: 0.4233\n",
      "\n",
      "\n",
      "-----------------------Random Forest Classifier-----------------------\n",
      "Accuracy: 0.2783\n",
      "Precision: 0.0346\n",
      "Recall: 0.0690\n",
      "F1 score: 0.0438\n",
      "Kappa score: 0.5358\n",
      "\n",
      "\n",
      "-----------------------Adaboost Classifier-----------------------\n",
      "Accuracy: 0.2586\n",
      "Precision: 0.0219\n",
      "Recall: 0.0808\n",
      "F1 score: 0.0344\n",
      "Kappa score: 0.5801\n",
      "\n",
      "\n",
      "-----------------------K Neibhors Classifier-----------------------\n",
      "Accuracy: 0.1304\n",
      "Precision: 0.0368\n",
      "Recall: 0.0853\n",
      "F1 score: 0.0412\n",
      "Kappa score: 0.2151\n",
      "\n",
      "\n",
      "-----------------------Support Vector Classifier-----------------------\n",
      "Accuracy: 0.2696\n",
      "Precision: 0.0118\n",
      "Recall: 0.0476\n",
      "F1 score: 0.0187\n",
      "Kappa score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------Logistic Regression-----------------------\")\n",
    "logistic_regression_qwk_set8 = cross_validation_function(embeddings_train, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Random Forest Classifier-----------------------\")\n",
    "random_forest_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"random_forest_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Adaboost Classifier-----------------------\")\n",
    "adaboost_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"adaboost_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------K Neibhors Classifier-----------------------\")\n",
    "k_neighbors_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"k_neighbors_classifier\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-----------------------Support Vector Classifier-----------------------\")\n",
    "support_vector_classifier_qwk_set8 = cross_validation_function(embeddings_train, y_train, ml_model = \"support_vector_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2b241964",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_qwk = [logistic_regression_qwk_set1, logistic_regression_qwk_set2, logistic_regression_qwk_set3,\n",
    "                           logistic_regression_qwk_set4, logistic_regression_qwk_set5, logistic_regression_qwk_set6,\n",
    "                           logistic_regression_qwk_set7, logistic_regression_qwk_set8]\n",
    "random_forest_classifier_qwk = [random_forest_classifier_qwk_set1, random_forest_classifier_qwk_set2, random_forest_classifier_qwk_set3,\n",
    "                                random_forest_classifier_qwk_set4, random_forest_classifier_qwk_set5, random_forest_classifier_qwk_set6,\n",
    "                                random_forest_classifier_qwk_set7, random_forest_classifier_qwk_set8]\n",
    "adaboost_classifier_qwk = [adaboost_classifier_qwk_set1, adaboost_classifier_qwk_set2, adaboost_classifier_qwk_set3,\n",
    "                           adaboost_classifier_qwk_set4, adaboost_classifier_qwk_set5, adaboost_classifier_qwk_set6,\n",
    "                           adaboost_classifier_qwk_set7, adaboost_classifier_qwk_set8]\n",
    "k_neighbors_classifier_qwk = [k_neighbors_classifier_qwk_set1, k_neighbors_classifier_qwk_set2, k_neighbors_classifier_qwk_set3,\n",
    "                              k_neighbors_classifier_qwk_set4, k_neighbors_classifier_qwk_set5, k_neighbors_classifier_qwk_set6,\n",
    "                              k_neighbors_classifier_qwk_set7, k_neighbors_classifier_qwk_set8]\n",
    "support_vector_classifier_qwk = [support_vector_classifier_qwk_set1, support_vector_classifier_qwk_set2, support_vector_classifier_qwk_set3,\n",
    "                                 support_vector_classifier_qwk_set4, support_vector_classifier_qwk_set5, support_vector_classifier_qwk_set6,\n",
    "                                 support_vector_classifier_qwk_set7, support_vector_classifier_qwk_set8]\n",
    "\n",
    "metrics_list = [logistic_regression_qwk, random_forest_classifier_qwk, adaboost_classifier_qwk,\n",
    "               k_neighbors_classifier_qwk, support_vector_classifier_qwk]\n",
    "\n",
    "results_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "results_df.rename(columns = {0: 'Prompt-1', 1: 'Prompt-2', 2: 'Prompt-3', \n",
    "                            3: 'Prompt-4', 4: 'Prompt-5', 5: 'Prompt-6',\n",
    "                            6: 'Prompt-7', 7: 'Prompt-8'}, inplace = True)\n",
    "\n",
    "results_df.rename(index = {0: 'GPT-2 + LR', 1: 'GPT-2 + RF', \n",
    "                           2: 'GPT-2 + Adaboost', 3: 'GPT-2 + KNN', 4: 'GPT-2 + SVC'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5ed215ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('Results/gpt-2 + features.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
