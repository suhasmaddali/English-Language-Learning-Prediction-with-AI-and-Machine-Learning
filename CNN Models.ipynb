{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "337f41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tensorflow.keras.layers import Input, GlobalMaxPooling1D, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2535ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0911b69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>pos_ratios</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>386</td>\n",
       "      <td>1875</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.031088082901554404, 'JJ': 0.05181347...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>0.385613</td>\n",
       "      <td>dear local newspaper think effect computer peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>2288</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.03879310344827586, ',': 0.0258620689...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.613167</td>\n",
       "      <td>dear believe using computer benefit u many way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>1541</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.04153354632587859, ',': 0.0287539936...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340393</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>dear people use computer everyone agrees benef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611</td>\n",
       "      <td>3165</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.11620294599018004, ',': 0.0212765957...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.441795</td>\n",
       "      <td>dear local newspaper found many expert say com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>517</td>\n",
       "      <td>2569</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.017408123791102514, ',': 0.025145067...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>dear know computer positive effect people comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  word_len  chars_len  \\\n",
       "0             NaN             NaN            NaN  ...       386       1875   \n",
       "1             NaN             NaN            NaN  ...       464       2288   \n",
       "2             NaN             NaN            NaN  ...       313       1541   \n",
       "3             NaN             NaN            NaN  ...       611       3165   \n",
       "4             NaN             NaN            NaN  ...       517       2569   \n",
       "\n",
       "   avg_word_length  avg_sentence_length  \\\n",
       "0         3.984456                  1.0   \n",
       "1         4.030172                  1.0   \n",
       "2         4.035144                  1.0   \n",
       "3         4.328969                  1.0   \n",
       "4         4.071567                  1.0   \n",
       "\n",
       "                                          pos_ratios  num_sentences  \\\n",
       "0  {'NNP': 0.031088082901554404, 'JJ': 0.05181347...             16   \n",
       "1  {'NNP': 0.03879310344827586, ',': 0.0258620689...             20   \n",
       "2  {'NNP': 0.04153354632587859, ',': 0.0287539936...             14   \n",
       "3  {'NNP': 0.11620294599018004, ',': 0.0212765957...             27   \n",
       "4  {'NNP': 0.017408123791102514, ',': 0.025145067...             30   \n",
       "\n",
       "   num_paragraphs  sentiment_polariy  sentiment_subjectivity  \\\n",
       "0               1           0.310471                0.385613   \n",
       "1               1           0.274000                0.613167   \n",
       "2               1           0.340393                0.498657   \n",
       "3               1           0.266828                0.441795   \n",
       "4               1           0.199684                0.485814   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear local newspaper think effect computer peo...  \n",
       "1  dear believe using computer benefit u many way...  \n",
       "2  dear people use computer everyone agrees benef...  \n",
       "3  dear local newspaper found many expert say com...  \n",
       "4  dear know computer positive effect people comp...  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f279fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c54c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7a2e9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the precision score between the true and predicted values\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calc_recall(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the recall score between the true and predicted values\n",
    "    \"\"\"\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calc_f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the f1-score between the true and predicted values\n",
    "    \"\"\"\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calc_cohen_kappa_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the cohen kappa score between the true and predicted values\n",
    "    \"\"\"\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calc_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy score between the true and predicted values\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e0caeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_function(y_actual, y_predictions):\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    accuracy = calc_accuracy(y_actual, y_predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Calculate and print precision\n",
    "    precision = calc_precision(y_actual, y_predictions)\n",
    "    print(\"Precision:\", precision)\n",
    "\n",
    "    # Calculate and print recall\n",
    "    recall = calc_recall(y_actual, y_predictions)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Calculate and print f1-score\n",
    "    f1 = calc_f1_score(y_actual, y_predictions)\n",
    "    print(\"F1-Score:\", f1)\n",
    "\n",
    "    # Calculate and print Cohen Kappa Score\n",
    "    kappa_score = calc_cohen_kappa_score(y_actual, y_predictions)\n",
    "    print(\"Cohen Kappa Score:\", kappa_score)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "843d18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set1 = df[df.essay_set == 1]\n",
    "df_essay_set2 = df[df.essay_set == 2]\n",
    "df_essay_set3 = df[df.essay_set == 3]\n",
    "df_essay_set4 = df[df.essay_set == 4]\n",
    "df_essay_set5 = df[df.essay_set == 5]\n",
    "df_essay_set6 = df[df.essay_set == 6]\n",
    "df_essay_set7 = df[df.essay_set == 7]\n",
    "df_essay_set8 = df[df.essay_set == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fc6dec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "354be444",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set1, y_set1 = dataset_preparation(df_essay_set1)\n",
    "X_set2, y_set2 = dataset_preparation(df_essay_set2)\n",
    "X_set3, y_set3 = dataset_preparation(df_essay_set3)\n",
    "X_set4, y_set4 = dataset_preparation(df_essay_set4)\n",
    "X_set5, y_set5 = dataset_preparation(df_essay_set5)\n",
    "X_set6, y_set6 = dataset_preparation(df_essay_set6)\n",
    "X_set7, y_set7 = dataset_preparation(df_essay_set7)\n",
    "X_set8, y_set8 = dataset_preparation(df_essay_set8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fe0a3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set1, X_test_set1, y_train_set1, y_test_set1 = train_test_split(X_set1, y_set1, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train_set2, X_test_set2, y_train_set2, y_test_set2 = train_test_split(X_set2, y_set2, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train_set3, X_test_set3, y_train_set3, y_test_set3 = train_test_split(X_set3, y_set3, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train_set4, X_test_set4, y_train_set4, y_test_set4 = train_test_split(X_set4, y_set4, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train_set5, X_test_set5, y_train_set5, y_test_set5 = train_test_split(X_set5, y_set5, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train_set6, X_test_set6, y_train_set6, y_test_set6 = train_test_split(X_set6, y_set6, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train_set7, X_test_set7, y_train_set7, y_test_set7 = train_test_split(X_set7, y_set7, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)\n",
    "\n",
    "X_train_set8, X_test_set8, y_train_set8, y_test_set8 = train_test_split(X_set8, y_set8, shuffle = True, \n",
    "                                                                       random_state = 101, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "318ca95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes(X_train, X_test, y_train, y_test, \n",
    "                 set_value = \"Essay Set-1\"):\n",
    "    \n",
    "    print(f\"------------------------{set_value}------------------------\")\n",
    "    print(\"The shape of input train data: {}\".format(X_train.shape))\n",
    "    print(\"The shape of input test data: {}\".format(X_test.shape))\n",
    "    print(\"The shape of output train data: {}\".format(y_train.shape))\n",
    "    print(\"The shape of output test data: {}\".format(y_test.shape))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f8ac7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Essay Set-1------------------------\n",
      "The shape of input train data: (1426, 12)\n",
      "The shape of input test data: (357, 12)\n",
      "The shape of output train data: (1426,)\n",
      "The shape of output test data: (357,)\n",
      "\n",
      "\n",
      "------------------------Essay Set-2------------------------\n",
      "The shape of input train data: (1440, 12)\n",
      "The shape of input test data: (360, 12)\n",
      "The shape of output train data: (1440,)\n",
      "The shape of output test data: (360,)\n",
      "\n",
      "\n",
      "------------------------Essay Set-3------------------------\n",
      "The shape of input train data: (1380, 12)\n",
      "The shape of input test data: (346, 12)\n",
      "The shape of output train data: (1380,)\n",
      "The shape of output test data: (346,)\n",
      "\n",
      "\n",
      "------------------------Essay Set-4------------------------\n",
      "The shape of input train data: (1416, 12)\n",
      "The shape of input test data: (354, 12)\n",
      "The shape of output train data: (1416,)\n",
      "The shape of output test data: (354,)\n",
      "\n",
      "\n",
      "------------------------Essay Set-5------------------------\n",
      "The shape of input train data: (1444, 12)\n",
      "The shape of input test data: (361, 12)\n",
      "The shape of output train data: (1444,)\n",
      "The shape of output test data: (361,)\n",
      "\n",
      "\n",
      "------------------------Essay Set-6------------------------\n",
      "The shape of input train data: (1440, 12)\n",
      "The shape of input test data: (360, 12)\n",
      "The shape of output train data: (1440,)\n",
      "The shape of output test data: (360,)\n",
      "\n",
      "\n",
      "------------------------Essay Set-7------------------------\n",
      "The shape of input train data: (1255, 12)\n",
      "The shape of input test data: (314, 12)\n",
      "The shape of output train data: (1255,)\n",
      "The shape of output test data: (314,)\n",
      "\n",
      "\n",
      "------------------------Essay Set-8------------------------\n",
      "The shape of input train data: (578, 12)\n",
      "The shape of input test data: (145, 12)\n",
      "The shape of output train data: (578,)\n",
      "The shape of output test data: (145,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_shapes(X_train_set1, X_test_set1, y_train_set1, y_test_set1)\n",
    "print_shapes(X_train_set2, X_test_set2, y_train_set2, y_test_set2, set_value = \"Essay Set-2\")\n",
    "print_shapes(X_train_set3, X_test_set3, y_train_set3, y_test_set3, set_value = \"Essay Set-3\")\n",
    "print_shapes(X_train_set4, X_test_set4, y_train_set4, y_test_set4, set_value = \"Essay Set-4\")\n",
    "print_shapes(X_train_set5, X_test_set5, y_train_set5, y_test_set5, set_value = \"Essay Set-5\")\n",
    "print_shapes(X_train_set6, X_test_set6, y_train_set6, y_test_set6, set_value = \"Essay Set-6\")\n",
    "print_shapes(X_train_set7, X_test_set7, y_train_set7, y_test_set7, set_value = \"Essay Set-7\")\n",
    "print_shapes(X_train_set8, X_test_set8, y_train_set8, y_test_set8, set_value = \"Essay Set-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17c34e",
   "metadata": {},
   "source": [
    "### Bag-of-Words Representation + Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "10422843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train_data, test_data, column = 'preprocessed_text'):\n",
    "    \n",
    "    X_train_corpus = train_data[column]\n",
    "    X_test_corpus = test_data[column]\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_bow = vectorizer.fit_transform(X_train_corpus)\n",
    "    X_test_bow = vectorizer.transform(X_test_corpus)\n",
    "    X_train_bow = pd.DataFrame(X_train_bow.toarray(), columns = vectorizer.get_feature_names())\n",
    "    X_test_bow = pd.DataFrame(X_test_bow.toarray(), columns = vectorizer.get_feature_names())\n",
    "    X_train_features = train_data.drop([column], axis = 1)\n",
    "    X_test_features = test_data.drop([column], axis = 1)\n",
    "    X_train_features.reset_index(drop = True, inplace = True)\n",
    "    X_test_features.reset_index(drop = True, inplace = True)\n",
    "    X_train_final = pd.concat([X_train_bow, X_train_features], axis = 1)\n",
    "    X_test_final = pd.concat([X_test_bow, X_test_features], axis = 1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_final = scaler.fit_transform(X_train_final)\n",
    "    X_test_final = scaler.transform(X_test_final)\n",
    "    \n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a78d7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final, X_test_final = feature_engineering(X_train_set1, X_test_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "75b74f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>386</td>\n",
       "      <td>1875</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>0.385613</td>\n",
       "      <td>dear local newspaper think effect computer peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>464</td>\n",
       "      <td>2288</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.613167</td>\n",
       "      <td>dear believe using computer benefit u many way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>313</td>\n",
       "      <td>1541</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340393</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>dear people use computer everyone agrees benef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>611</td>\n",
       "      <td>3165</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.441795</td>\n",
       "      <td>dear local newspaper found many expert say com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>517</td>\n",
       "      <td>2569</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>dear know computer positive effect people comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set  rater1_domain1  rater2_domain1  domain1_score  word_len  \\\n",
       "0          1               4               4              8       386   \n",
       "1          1               5               4              9       464   \n",
       "2          1               4               3              7       313   \n",
       "3          1               5               5             10       611   \n",
       "4          1               4               4              8       517   \n",
       "\n",
       "   chars_len  avg_word_length  avg_sentence_length  num_sentences  \\\n",
       "0       1875         3.984456                  1.0             16   \n",
       "1       2288         4.030172                  1.0             20   \n",
       "2       1541         4.035144                  1.0             14   \n",
       "3       3165         4.328969                  1.0             27   \n",
       "4       2569         4.071567                  1.0             30   \n",
       "\n",
       "   num_paragraphs  sentiment_polariy  sentiment_subjectivity  \\\n",
       "0               1           0.310471                0.385613   \n",
       "1               1           0.274000                0.613167   \n",
       "2               1           0.340393                0.498657   \n",
       "3               1           0.266828                0.441795   \n",
       "4               1           0.199684                0.485814   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear local newspaper think effect computer peo...  \n",
       "1  dear believe using computer benefit u many way...  \n",
       "2  dear people use computer everyone agrees benef...  \n",
       "3  dear local newspaper found many expert say com...  \n",
       "4  dear know computer positive effect people comp...  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3152eac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['essay_set', 'rater1_domain1', 'rater2_domain1', 'domain1_score',\n",
       "       'word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
       "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
       "       'sentiment_subjectivity', 'preprocessed_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cff61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "30d0699b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1346    dear local people using computer year good hea...\n",
       "1349    dear newspaper believe computer positive affec...\n",
       "7       people agree computer make life le complicated...\n",
       "1251    dear world changed much better technology beco...\n",
       "661     technology growing changing rapidly look apple...\n",
       "                              ...                        \n",
       "599     dear know becoming reaching computer helpful m...\n",
       "1599    although many people love computer palying vid...\n",
       "1361    dear local newspaper think computer bad effect...\n",
       "1547    dear local newspaper agree expert said compute...\n",
       "863     dear local newspaper opinion computer everythi...\n",
       "Name: preprocessed_text, Length: 1426, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_set1['preprocessed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "721940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_kappa(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the quadratic Cohen's kappa score for two arrays of labels\n",
    "    \"\"\"\n",
    "    # Convert predicted probabilities to integer labels\n",
    "    y_pred_labels = tf.math.argmax(y_pred, axis=1)\n",
    "    y_true_labels = tf.math.argmax(y_true, axis=1)\n",
    "    \n",
    "    # Calculate the quadratic kappa score\n",
    "    kappa = cohen_kappa_score(y_true_labels, y_pred_labels, weights='quadratic')\n",
    "    \n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24646f",
   "metadata": {},
   "source": [
    "### CNN Configuration - 1 Model (Essay Set-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "28eab745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1426 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_sentences  num_paragraphs\n",
       "1346             37               1\n",
       "1349             15               1\n",
       "7                39               1\n",
       "1251             31               1\n",
       "661              26               1\n",
       "...             ...             ...\n",
       "599              10               1\n",
       "1599             18               1\n",
       "1361             18               1\n",
       "1547             12               1\n",
       "863              17               1\n",
       "\n",
       "[1426 rows x 2 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a143c98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_set1.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f196ed24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(y_train_set1).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "56496dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 500)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e4bffb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 13)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_set1_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed9406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e1e87ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12788"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1df9330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 500)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9959651b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 500)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "81886df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 13)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_set1_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1c82da0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_42 (Embedding)     (None, 500, 100)          1278900   \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 499, 10)           2010      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_29 (Glo (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 13)                221       \n",
      "=================================================================\n",
      "Total params: 1,281,307\n",
      "Trainable params: 1,281,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 1.8801 - accuracy: 0.3612 - val_loss: 1.7471 - val_accuracy: 0.3641\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.6750 - accuracy: 0.3843 - val_loss: 1.7476 - val_accuracy: 0.3165\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.5961 - accuracy: 0.3983 - val_loss: 1.7111 - val_accuracy: 0.3613\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4684 - accuracy: 0.4600 - val_loss: 1.6650 - val_accuracy: 0.3473\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3705 - accuracy: 0.4888 - val_loss: 1.6380 - val_accuracy: 0.3697\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 1.2069 - accuracy: 0.5259 - val_loss: 1.8180 - val_accuracy: 0.3221\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.1126 - accuracy: 0.5729 - val_loss: 1.8956 - val_accuracy: 0.3501\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 0.9873 - accuracy: 0.6290 - val_loss: 1.8819 - val_accuracy: 0.3417\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.8531 - accuracy: 0.6907 - val_loss: 2.2161 - val_accuracy: 0.2689\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.8426 - accuracy: 0.6914 - val_loss: 2.1751 - val_accuracy: 0.3165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x209de55ad90>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_set1['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train_set1['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test_set1['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 500\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "y_train_set1_categorical = to_categorical(y_train_set1)\n",
    "y_test_set1_categorical = to_categorical(y_test_set1)\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=2, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train_set1_categorical.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train, y_train_set1_categorical, epochs=10, batch_size=32, validation_data=(padded_sequences_test, y_test_set1_categorical))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "aeb5d6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the training and validation loss values from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation loss curves with scatter points\n",
    "plt.plot(train_loss, color='green', label='Training Loss')\n",
    "plt.plot(val_loss, color='orange', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4500cb5",
   "metadata": {},
   "source": [
    "### CNN Configuration - 2 Model (Cohesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d402bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_cohesion)\n",
    "labels_test = np.array(y_test_cohesion)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f82de0",
   "metadata": {},
   "source": [
    "### CNN Configuration - 3 Model (Cohesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4438dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_cohesion)\n",
    "labels_test = np.array(y_test_cohesion)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e91bf4",
   "metadata": {},
   "source": [
    "### CNN Configuration - 4 Model (Cohesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b33753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_cohesion)\n",
    "labels_test = np.array(y_test_cohesion)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671bc87",
   "metadata": {},
   "source": [
    "### CNN Configuration - 5 Model (Cohesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_cohesion)\n",
    "labels_test = np.array(y_test_cohesion)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc735f21",
   "metadata": {},
   "source": [
    "### CNN Configuration - 6 Model (Cohesion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_cohesion)\n",
    "labels_test = np.array(y_test_cohesion)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db523653",
   "metadata": {},
   "source": [
    "### CNN Configuration - 1 Model (Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_syntax)\n",
    "labels_test = np.array(y_test_syntax)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4d4bf",
   "metadata": {},
   "source": [
    "### CNN Configuration - 2 Model (Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_syntax)\n",
    "labels_test = np.array(y_test_syntax)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e1ab9",
   "metadata": {},
   "source": [
    "### CNN Configuration - 3 Model (Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c606d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_syntax)\n",
    "labels_test = np.array(y_test_syntax)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a324a4",
   "metadata": {},
   "source": [
    "### CNN Configuration - 4 Model (Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_syntax)\n",
    "labels_test = np.array(y_test_syntax)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c93f55",
   "metadata": {},
   "source": [
    "### CNN Configuration - 5 Model (Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabe1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_syntax)\n",
    "labels_test = np.array(y_test_syntax)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca651ac8",
   "metadata": {},
   "source": [
    "### CNN Configuration - 6 Model (Syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_syntax)\n",
    "labels_test = np.array(y_test_syntax)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a393ccc",
   "metadata": {},
   "source": [
    "### CNN Configuration - 1 Model (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada88e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_vocabulary)\n",
    "labels_test = np.array(y_test_vocabulary)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94bd2b",
   "metadata": {},
   "source": [
    "### CNN Configuration - 2 Model (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_vocabulary)\n",
    "labels_test = np.array(y_test_vocabulary)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580df952",
   "metadata": {},
   "source": [
    "### CNN Configuration - 3 Model (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_vocabulary)\n",
    "labels_test = np.array(y_test_vocabulary)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ef6f2",
   "metadata": {},
   "source": [
    "### CNN Configuration - 4 Model (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c03f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_vocabulary)\n",
    "labels_test = np.array(y_test_vocabulary)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea3a26",
   "metadata": {},
   "source": [
    "### CNN Configuration - 5 Model (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_vocabulary)\n",
    "labels_test = np.array(y_test_vocabulary)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f05bd8",
   "metadata": {},
   "source": [
    "### CNN Configuration - 6 Model (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15559b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_vocabulary)\n",
    "labels_test = np.array(y_test_vocabulary)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6e896",
   "metadata": {},
   "source": [
    "### CNN Configuration - 1 Model (Phraseology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_phraseology)\n",
    "labels_test = np.array(y_test_phraseology)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cea744",
   "metadata": {},
   "source": [
    "### CNN Configuration - 2 Model (Phraseology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bf0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_phraseology)\n",
    "labels_test = np.array(y_test_phraseology)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb6260",
   "metadata": {},
   "source": [
    "### CNN Configuration - 3 Model (Phraseology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_phraseology)\n",
    "labels_test = np.array(y_test_phraseology)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431ceed",
   "metadata": {},
   "source": [
    "### CNN Configuration - 4 Model (Phraseology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_phraseology)\n",
    "labels_test = np.array(y_test_phraseology)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b57e67a",
   "metadata": {},
   "source": [
    "### CNN Configuration - 5 Model (Phraseology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdef71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_phraseology)\n",
    "labels_test = np.array(y_test_phraseology)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c00a0c",
   "metadata": {},
   "source": [
    "### CNN Configuration - 6 Model (Phraseology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_phraseology)\n",
    "labels_test = np.array(y_test_phraseology)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2d668",
   "metadata": {},
   "source": [
    "### CNN Configuration - 1 Model (Grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d447b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_grammar)\n",
    "labels_test = np.array(y_test_grammar)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c841d",
   "metadata": {},
   "source": [
    "### CNN Configuration - 2 Model (Grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_grammar)\n",
    "labels_test = np.array(y_test_grammar)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8281534",
   "metadata": {},
   "source": [
    "### CNN Configuration - 3 Model (Grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05489be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_grammar)\n",
    "labels_test = np.array(y_test_grammar)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307d1bc",
   "metadata": {},
   "source": [
    "### CNN Configuration - 4 Model (Grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dde1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_grammar)\n",
    "labels_test = np.array(y_test_grammar)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850de6d",
   "metadata": {},
   "source": [
    "### CNN Configuration - 5 Model (Grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d54e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_grammar)\n",
    "labels_test = np.array(y_test_grammar)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885c0a8",
   "metadata": {},
   "source": [
    "### CNN Configuration - 6 Model (Grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a60471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_grammar)\n",
    "labels_test = np.array(y_test_grammar)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fae378",
   "metadata": {},
   "source": [
    "### CNN Configuration - 1 Model (Conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_conventions)\n",
    "labels_test = np.array(y_test_conventions)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852ba98",
   "metadata": {},
   "source": [
    "### CNN Configuration - 2 Model (Conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419991a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_conventions)\n",
    "labels_test = np.array(y_test_conventions)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb8298",
   "metadata": {},
   "source": [
    "### CNN Configuration - 3 Model (Conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76322f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_conventions)\n",
    "labels_test = np.array(y_test_conventions)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d87a0",
   "metadata": {},
   "source": [
    "### CNN Configuration - 4 Model (Conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c19a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_conventions)\n",
    "labels_test = np.array(y_test_conventions)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34b20c",
   "metadata": {},
   "source": [
    "### CNN Configuration - 5 Model (Conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_conventions)\n",
    "labels_test = np.array(y_test_conventions)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922f990",
   "metadata": {},
   "source": [
    "### CNN Configuration - 6 Model (Conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda29327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = len(max(sequences_train))\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen = max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen = max_length)\n",
    "\n",
    "# Convert the labels to a NumPy array\n",
    "labels_train = np.array(y_train_conventions)\n",
    "labels_test = np.array(y_test_conventions)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8, input_length=max_length))\n",
    "model.add(Conv1D(filters=1, kernel_size=20, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.95))\n",
    "model.add(MaxPooling1D(pool_size=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 0.0012)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences_train, labels_train, epochs=15, batch_size=64,\n",
    "                   validation_data = (padded_sequences_test, labels_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
