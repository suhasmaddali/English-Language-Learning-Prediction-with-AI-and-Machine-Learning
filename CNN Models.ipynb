{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "337f41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tensorflow.keras.layers import Input, GlobalMaxPooling1D, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2535ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ASAP Dataset/Preprocessed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0911b69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>word_len</th>\n",
       "      <th>chars_len</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>pos_ratios</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>sentiment_polariy</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>386</td>\n",
       "      <td>1875</td>\n",
       "      <td>3.984456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.031088082901554404, 'JJ': 0.05181347...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310471</td>\n",
       "      <td>0.385613</td>\n",
       "      <td>dear local newspaper think effect computer peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>2288</td>\n",
       "      <td>4.030172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.03879310344827586, ',': 0.0258620689...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.613167</td>\n",
       "      <td>dear believe using computer benefit u many way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>1541</td>\n",
       "      <td>4.035144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.04153354632587859, ',': 0.0287539936...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340393</td>\n",
       "      <td>0.498657</td>\n",
       "      <td>dear people use computer everyone agrees benef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611</td>\n",
       "      <td>3165</td>\n",
       "      <td>4.328969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.11620294599018004, ',': 0.0212765957...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.441795</td>\n",
       "      <td>dear local newspaper found many expert say com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>517</td>\n",
       "      <td>2569</td>\n",
       "      <td>4.071567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'NNP': 0.017408123791102514, ',': 0.025145067...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199684</td>\n",
       "      <td>0.485814</td>\n",
       "      <td>dear know computer positive effect people comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  word_len  chars_len  \\\n",
       "0             NaN             NaN            NaN  ...       386       1875   \n",
       "1             NaN             NaN            NaN  ...       464       2288   \n",
       "2             NaN             NaN            NaN  ...       313       1541   \n",
       "3             NaN             NaN            NaN  ...       611       3165   \n",
       "4             NaN             NaN            NaN  ...       517       2569   \n",
       "\n",
       "   avg_word_length  avg_sentence_length  \\\n",
       "0         3.984456                  1.0   \n",
       "1         4.030172                  1.0   \n",
       "2         4.035144                  1.0   \n",
       "3         4.328969                  1.0   \n",
       "4         4.071567                  1.0   \n",
       "\n",
       "                                          pos_ratios  num_sentences  \\\n",
       "0  {'NNP': 0.031088082901554404, 'JJ': 0.05181347...             16   \n",
       "1  {'NNP': 0.03879310344827586, ',': 0.0258620689...             20   \n",
       "2  {'NNP': 0.04153354632587859, ',': 0.0287539936...             14   \n",
       "3  {'NNP': 0.11620294599018004, ',': 0.0212765957...             27   \n",
       "4  {'NNP': 0.017408123791102514, ',': 0.025145067...             30   \n",
       "\n",
       "   num_paragraphs  sentiment_polariy  sentiment_subjectivity  \\\n",
       "0               1           0.310471                0.385613   \n",
       "1               1           0.274000                0.613167   \n",
       "2               1           0.340393                0.498657   \n",
       "3               1           0.266828                0.441795   \n",
       "4               1           0.199684                0.485814   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear local newspaper think effect computer peo...  \n",
       "1  dear believe using computer benefit u many way...  \n",
       "2  dear people use computer everyone agrees benef...  \n",
       "3  dear local newspaper found many expert say com...  \n",
       "4  dear know computer positive effect people comp...  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f279fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 1, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "295a9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['essay_id', 'pos_ratios', 'essay', 'rater1_domain1', 'rater2_domain1']\n",
    "df.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f78d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the precision score between the true and predicted values\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    return precision\n",
    "\n",
    "def calc_recall(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the recall score between the true and predicted values\n",
    "    \"\"\"\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    return recall\n",
    "\n",
    "def calc_f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Calculates the f1-score between the true and predicted values\n",
    "    \"\"\"\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    return f1\n",
    "\n",
    "def calc_cohen_kappa_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the cohen kappa score between the true and predicted values\n",
    "    \"\"\"\n",
    "    kappa_score = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n",
    "    return kappa_score\n",
    "\n",
    "def calc_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy score between the true and predicted values\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0caeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_function(y_actual, y_predictions):\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    accuracy = calc_accuracy(y_actual, y_predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    # Calculate and print precision\n",
    "    precision = calc_precision(y_actual, y_predictions)\n",
    "    print(\"Precision:\", precision)\n",
    "\n",
    "    # Calculate and print recall\n",
    "    recall = calc_recall(y_actual, y_predictions)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    # Calculate and print f1-score\n",
    "    f1 = calc_f1_score(y_actual, y_predictions)\n",
    "    print(\"F1-Score:\", f1)\n",
    "\n",
    "    # Calculate and print Cohen Kappa Score\n",
    "    kappa_score = calc_cohen_kappa_score(y_actual, y_predictions)\n",
    "    print(\"Cohen Kappa Score:\", kappa_score)\n",
    "\n",
    "    return accuracy, precision, recall, f1, kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8adf686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "731503a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_corrector(tokens):\n",
    "    spell_checker = SpellChecker()\n",
    "    correct_tokens = []\n",
    "    for token in tqdm(tokens):\n",
    "        if spell_checker.correction(token.lower()):\n",
    "            correct_tokens.append(spell_checker.correction(token.lower()))\n",
    "        else:\n",
    "            correct_tokens.append(token.lower())\n",
    "    \n",
    "    return ' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "893a1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, target = 'domain1_score'):\n",
    "    \n",
    "    X = data.drop([target], axis = 1)\n",
    "    y = data[target]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd61943",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0abaab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 1]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d372b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c08b213c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "44/45 [============================>.] - ETA: 0s - loss: 2.0762 - accuracy: 0.3359WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 2.0735 - accuracy: 0.3359 - val_loss: 1.7567 - val_accuracy: 0.3641\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 1.6669 - accuracy: 0.4081 - val_loss: 1.6886 - val_accuracy: 0.3950\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4447 - accuracy: 0.4993 - val_loss: 1.6504 - val_accuracy: 0.3838\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.1688 - accuracy: 0.5968 - val_loss: 1.6582 - val_accuracy: 0.3866\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.9226 - accuracy: 0.6914 - val_loss: 1.7712 - val_accuracy: 0.3866\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.7506 - accuracy: 0.7300 - val_loss: 1.8452 - val_accuracy: 0.3866\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.6340 - accuracy: 0.7896 - val_loss: 1.9863 - val_accuracy: 0.3866\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.5241 - accuracy: 0.8226 - val_loss: 2.1098 - val_accuracy: 0.3585\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.4350 - accuracy: 0.8492 - val_loss: 2.1217 - val_accuracy: 0.3445\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.3794 - accuracy: 0.8668 - val_loss: 2.1883 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5680dddf0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52a0c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.13276675474969002\n",
      "Recall: 0.16239541708291708\n",
      "F1-Score: 0.14241225605270294\n",
      "Cohen Kappa Score: 0.4073113882641587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333,\n",
       " 0.13276675474969002,\n",
       " 0.16239541708291708,\n",
       " 0.14241225605270294,\n",
       " 0.4073113882641587)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbca46",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2842c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 2]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3f6612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c00d859e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "44/45 [============================>.] - ETA: 0s - loss: 2.0762 - accuracy: 0.3359WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 2.0735 - accuracy: 0.3359 - val_loss: 1.7567 - val_accuracy: 0.3641\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 1.6669 - accuracy: 0.4081 - val_loss: 1.6886 - val_accuracy: 0.3950\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4447 - accuracy: 0.4993 - val_loss: 1.6504 - val_accuracy: 0.3838\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.1688 - accuracy: 0.5968 - val_loss: 1.6582 - val_accuracy: 0.3866\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.9226 - accuracy: 0.6914 - val_loss: 1.7712 - val_accuracy: 0.3866\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.7506 - accuracy: 0.7300 - val_loss: 1.8452 - val_accuracy: 0.3866\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.6340 - accuracy: 0.7896 - val_loss: 1.9863 - val_accuracy: 0.3866\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.5241 - accuracy: 0.8226 - val_loss: 2.1098 - val_accuracy: 0.3585\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.4350 - accuracy: 0.8492 - val_loss: 2.1217 - val_accuracy: 0.3445\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.3794 - accuracy: 0.8668 - val_loss: 2.1883 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5680dddf0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88349795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.13276675474969002\n",
      "Recall: 0.16239541708291708\n",
      "F1-Score: 0.14241225605270294\n",
      "Cohen Kappa Score: 0.4073113882641587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333,\n",
       " 0.13276675474969002,\n",
       " 0.16239541708291708,\n",
       " 0.14241225605270294,\n",
       " 0.4073113882641587)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45e6d2",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0bbf1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 3]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37539c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e86b2b88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_6:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (32, 758).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_6:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (32, 758).\n",
      "42/45 [===========================>..] - ETA: 0s - loss: 1.3758 - accuracy: 0.4301WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_6:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3508 - accuracy: 0.4347 - val_loss: 1.1046 - val_accuracy: 0.4944\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.0444 - accuracy: 0.5583 - val_loss: 1.0365 - val_accuracy: 0.5500\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.8699 - accuracy: 0.6438 - val_loss: 1.0733 - val_accuracy: 0.5194\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.6635 - accuracy: 0.7583 - val_loss: 1.0985 - val_accuracy: 0.5333\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.4461 - accuracy: 0.8396 - val_loss: 1.2365 - val_accuracy: 0.5194\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.3407 - accuracy: 0.8840 - val_loss: 1.1835 - val_accuracy: 0.5528\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.2360 - accuracy: 0.9181 - val_loss: 1.2290 - val_accuracy: 0.5472\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.2228 - accuracy: 0.9201 - val_loss: 1.2910 - val_accuracy: 0.5472\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.2000 - accuracy: 0.9333 - val_loss: 1.4428 - val_accuracy: 0.5333\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.2020 - accuracy: 0.9264 - val_loss: 1.3704 - val_accuracy: 0.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b577576e50>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94e1829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_6:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.5333333333333333\n",
      "Precision: 0.41988319273199226\n",
      "Recall: 0.31073339231931074\n",
      "F1-Score: 0.3278568922005577\n",
      "Cohen Kappa Score: 0.4396887159533074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5333333333333333,\n",
       " 0.41988319273199226,\n",
       " 0.31073339231931074,\n",
       " 0.3278568922005577,\n",
       " 0.4396887159533074)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27204514",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2fb97cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 4]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf7b9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b18e14f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"input_13:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 1008).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"input_13:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 1008).\n",
      "37/44 [========================>.....] - ETA: 0s - loss: 1.2022 - accuracy: 0.3623"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  indices[4,1006] = -1 is not in [0, 5423)\n\t [[node functional_21/embedding_12/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4016339990.py:36) ]]\n\t [[functional_21/embedding_12/embedding_lookup/_24]]\n  (1) Invalid argument:  indices[4,1006] = -1 is not in [0, 5423)\n\t [[node functional_21/embedding_12/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4016339990.py:36) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_17808]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_21/embedding_12/embedding_lookup:\n functional_21/embedding_12/embedding_lookup/17500 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nInput Source operations connected to node functional_21/embedding_12/embedding_lookup:\n functional_21/embedding_12/embedding_lookup/17500 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m padded_sequences_test_concat \u001b[38;5;241m=\u001b[39m concatenate([padded_sequences_test, X_test_additional_features])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences_train_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences_test_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1098\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraceContext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1093\u001b[0m     graph_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1094\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1095\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1096\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   1097\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1098\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1100\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py:807\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    804\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    805\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    806\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    810\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:2829\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2828\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filtered_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:1843\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filtered_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs, cancellation_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1828\u001b[0m   \u001b[38;5;124;03m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \n\u001b[0;32m   1830\u001b[0m \u001b[38;5;124;03m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;124;03m    `args` and `kwargs`.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseResourceVariable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:1923\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1918\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1919\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1921\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1922\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1925\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m     args,\n\u001b[0;32m   1927\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1928\u001b[0m     executing_eagerly)\n\u001b[0;32m   1929\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:545\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    554\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    558\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  indices[4,1006] = -1 is not in [0, 5423)\n\t [[node functional_21/embedding_12/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4016339990.py:36) ]]\n\t [[functional_21/embedding_12/embedding_lookup/_24]]\n  (1) Invalid argument:  indices[4,1006] = -1 is not in [0, 5423)\n\t [[node functional_21/embedding_12/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4016339990.py:36) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_17808]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_21/embedding_12/embedding_lookup:\n functional_21/embedding_12/embedding_lookup/17500 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nInput Source operations connected to node functional_21/embedding_12/embedding_lookup:\n functional_21/embedding_12/embedding_lookup/17500 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 1000\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=2, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af4051cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.13276675474969002\n",
      "Recall: 0.16239541708291708\n",
      "F1-Score: 0.14241225605270294\n",
      "Cohen Kappa Score: 0.4073113882641587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333,\n",
       " 0.13276675474969002,\n",
       " 0.16239541708291708,\n",
       " 0.14241225605270294,\n",
       " 0.4073113882641587)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984c08a",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "954ced84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 5]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "638645c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2b2a7320",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_14:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_14:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2825 - accuracy: 0.4032WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_14:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 1.2825 - accuracy: 0.4032 - val_loss: 1.1777 - val_accuracy: 0.4548\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.0447 - accuracy: 0.5304 - val_loss: 0.9792 - val_accuracy: 0.5367\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.7973 - accuracy: 0.6737 - val_loss: 0.9219 - val_accuracy: 0.5904\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.5951 - accuracy: 0.7641 - val_loss: 1.0196 - val_accuracy: 0.5565\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 0.4602 - accuracy: 0.8333 - val_loss: 1.1153 - val_accuracy: 0.5706\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.3089 - accuracy: 0.8849 - val_loss: 1.4884 - val_accuracy: 0.5480\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.2945 - accuracy: 0.9103 - val_loss: 1.3347 - val_accuracy: 0.5452\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.1956 - accuracy: 0.9428 - val_loss: 1.4714 - val_accuracy: 0.5819\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 0.1711 - accuracy: 0.9470 - val_loss: 1.5278 - val_accuracy: 0.5791\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.1670 - accuracy: 0.9484 - val_loss: 1.7329 - val_accuracy: 0.5198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5ad772d00>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8be809a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_14:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.519774011299435\n",
      "Precision: 0.5171502914337018\n",
      "Recall: 0.49888738527500504\n",
      "F1-Score: 0.5004295459696358\n",
      "Cohen Kappa Score: 0.6503452243958574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.519774011299435,\n",
       " 0.5171502914337018,\n",
       " 0.49888738527500504,\n",
       " 0.5004295459696358,\n",
       " 0.6503452243958574)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0862e7",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00488f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 6]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31d1c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5883f7c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_15:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_15:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "46/46 [==============================] - ETA: 0s - loss: 1.4963 - accuracy: 0.3096WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_15:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "46/46 [==============================] - 1s 23ms/step - loss: 1.4963 - accuracy: 0.3096 - val_loss: 1.2844 - val_accuracy: 0.4072\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.2152 - accuracy: 0.4425 - val_loss: 1.1598 - val_accuracy: 0.4737\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 1.0352 - accuracy: 0.5512 - val_loss: 1.1163 - val_accuracy: 0.4543\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 0s 11ms/step - loss: 0.8582 - accuracy: 0.6510 - val_loss: 1.0983 - val_accuracy: 0.4958\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.7157 - accuracy: 0.6863 - val_loss: 1.1275 - val_accuracy: 0.5097\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.5828 - accuracy: 0.7729 - val_loss: 1.1319 - val_accuracy: 0.5319\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.5109 - accuracy: 0.8151 - val_loss: 1.2081 - val_accuracy: 0.5125\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.4353 - accuracy: 0.8463 - val_loss: 1.2588 - val_accuracy: 0.5429\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 1s 11ms/step - loss: 0.3746 - accuracy: 0.8594 - val_loss: 1.3792 - val_accuracy: 0.5319\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 0.3189 - accuracy: 0.8843 - val_loss: 1.4604 - val_accuracy: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5ad6adb80>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7f432c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_15:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.5346260387811634\n",
      "Precision: 0.41769759450171823\n",
      "Recall: 0.4458085458886206\n",
      "F1-Score: 0.42794265718441943\n",
      "Cohen Kappa Score: 0.6356448967576434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5346260387811634,\n",
       " 0.41769759450171823,\n",
       " 0.4458085458886206,\n",
       " 0.42794265718441943,\n",
       " 0.6356448967576434)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad43e5",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "80e41a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 7]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4e2d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc314215",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_16:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (32, 758).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_16:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (32, 758).\n",
      "41/45 [==========================>...] - ETA: 0s - loss: 1.3614 - accuracy: 0.4261WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_16:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.3515 - accuracy: 0.4215 - val_loss: 1.1457 - val_accuracy: 0.4917\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.1401 - accuracy: 0.4868 - val_loss: 1.0831 - val_accuracy: 0.5056\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.0297 - accuracy: 0.5597 - val_loss: 1.0157 - val_accuracy: 0.5333\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.9509 - accuracy: 0.5924 - val_loss: 1.0014 - val_accuracy: 0.5500\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.8200 - accuracy: 0.6549 - val_loss: 1.0440 - val_accuracy: 0.5111\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.6941 - accuracy: 0.7146 - val_loss: 1.0260 - val_accuracy: 0.5278\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.5652 - accuracy: 0.7736 - val_loss: 1.1036 - val_accuracy: 0.5306\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.4588 - accuracy: 0.8229 - val_loss: 1.2287 - val_accuracy: 0.4944\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 0.3930 - accuracy: 0.8382 - val_loss: 1.3026 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.3382 - accuracy: 0.8653 - val_loss: 1.4060 - val_accuracy: 0.4944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5a4ae0340>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4aa160d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_16:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.49444444444444446\n",
      "Precision: 0.3984834302794085\n",
      "Recall: 0.3670094837724896\n",
      "F1-Score: 0.37290524272114906\n",
      "Cohen Kappa Score: 0.5657507360157017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.49444444444444446,\n",
       " 0.3984834302794085,\n",
       " 0.3670094837724896,\n",
       " 0.37290524272114906,\n",
       " 0.5657507360157017)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d6ada",
   "metadata": {},
   "source": [
    "### Model with Metrics (Essay Set - 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c15627a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_essay_set = df[df.essay_set == 8]\n",
    "X, y = dataset_preparation(df_essay_set)\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, \n",
    "                                                    random_state = 101, test_size = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a5275e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78a2a4b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_17:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_17:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "25/40 [=================>............] - ETA: 0s - loss: 3.0781 - accuracy: 0.0700"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  indices[0,756] = -1 is not in [0, 8333)\n\t [[node functional_29/embedding_16/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4194731421.py:36) ]]\n\t [[functional_29/embedding_16/embedding_lookup/_24]]\n  (1) Invalid argument:  indices[0,756] = -1 is not in [0, 8333)\n\t [[node functional_29/embedding_16/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4194731421.py:36) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_26571]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_29/embedding_16/embedding_lookup:\n functional_29/embedding_16/embedding_lookup/26263 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nInput Source operations connected to node functional_29/embedding_16/embedding_lookup:\n functional_29/embedding_16/embedding_lookup/26263 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m padded_sequences_test_concat \u001b[38;5;241m=\u001b[39m concatenate([padded_sequences_test, X_test_additional_features])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences_train_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences_test_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1098\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraceContext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1093\u001b[0m     graph_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1094\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1095\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1096\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   1097\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1098\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1100\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py:807\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    804\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    805\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    806\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    810\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:2829\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2828\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filtered_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:1843\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filtered_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs, cancellation_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1828\u001b[0m   \u001b[38;5;124;03m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \n\u001b[0;32m   1830\u001b[0m \u001b[38;5;124;03m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;124;03m    `args` and `kwargs`.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseResourceVariable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:1923\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1918\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1919\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1921\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1922\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1925\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m     args,\n\u001b[0;32m   1927\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1928\u001b[0m     executing_eagerly)\n\u001b[0;32m   1929\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py:545\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    554\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    558\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  indices[0,756] = -1 is not in [0, 8333)\n\t [[node functional_29/embedding_16/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4194731421.py:36) ]]\n\t [[functional_29/embedding_16/embedding_lookup/_24]]\n  (1) Invalid argument:  indices[0,756] = -1 is not in [0, 8333)\n\t [[node functional_29/embedding_16/embedding_lookup (defined at Users\\suhas maddali\\AppData\\Local\\Temp\\ipykernel_4016\\4194731421.py:36) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_26571]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_29/embedding_16/embedding_lookup:\n functional_29/embedding_16/embedding_lookup/26263 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nInput Source operations connected to node functional_29/embedding_16/embedding_lookup:\n functional_29/embedding_16/embedding_lookup/26263 (defined at Anaconda_latest\\envs\\englishlanguagelearning_gpu\\lib\\contextlib.py:113)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train['preprocessed_text'])\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train['preprocessed_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test['preprocessed_text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 750\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_length)\n",
    "\n",
    "X_train_additional_features = X_train[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "X_test_additional_features = X_test[['word_len', 'chars_len', 'avg_word_length', 'avg_sentence_length',\n",
    "       'num_sentences', 'num_paragraphs', 'sentiment_polariy',\n",
    "       'sentiment_subjectivity']].values.astype('int32')\n",
    "\n",
    "# Define the model architecture\n",
    "text_input = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(text_input)\n",
    "conv_layer = Conv1D(filters=10, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "dropout_layer = Dropout(0.2)(pooling_layer)\n",
    "dense_layer1 = Dense(units=16, activation='relu')(dropout_layer)\n",
    "output_layer = Dense(units=y_train.shape[1], activation='softmax')(dense_layer1)\n",
    "model = Model(inputs = text_input, outputs = output_layer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "padded_sequences_train_concat = concatenate([padded_sequences_train, X_train_additional_features])\n",
    "padded_sequences_test_concat = concatenate([padded_sequences_test, X_test_additional_features])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_train_concat, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_test_concat, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e30abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 750) for input Tensor(\"input_5:0\", shape=(None, 750), dtype=float32), but it was called on an input with incompatible shape (None, 758).\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.13276675474969002\n",
      "Recall: 0.16239541708291708\n",
      "F1-Score: 0.14241225605270294\n",
      "Cohen Kappa Score: 0.4073113882641587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333,\n",
       " 0.13276675474969002,\n",
       " 0.16239541708291708,\n",
       " 0.14241225605270294,\n",
       " 0.4073113882641587)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions = model.predict(padded_sequences_test_concat)\n",
    "y_predictions = np.argmax(y_predictions, axis = 1)\n",
    "print_metrics_function(np.argmax(y_test, axis = 1), y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf1c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
