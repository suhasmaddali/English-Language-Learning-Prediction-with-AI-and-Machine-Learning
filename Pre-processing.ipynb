{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VENKATESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VENKATESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VENKATESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import important libraries and download data\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import multiprocessing\n",
    "# %matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  \n",
    "# ! git clone https://github.com/Gaurav-Pande/AES_DL.git && mv AES_DL/data .\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring some visualization methods to plot accuracy and model diagram\n",
    "def plot_accuracy_curve(history):\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['mae'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "def plot_acrchitecture(filename, model):\n",
    "  from keras.utils import plot_model\n",
    "  plot_model(model, to_file=str(filename) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to split data into sets\n",
    "def split_in_sets(data):\n",
    "    essay_sets = []\n",
    "    min_scores = []\n",
    "    max_scores = []\n",
    "    for s in range(1,9):\n",
    "        essay_set = data[data[\"essay_set\"] == s]\n",
    "        essay_set.dropna(axis=1, inplace=True)\n",
    "        n, d = essay_set.shape\n",
    "        set_scores = essay_set[\"domain1_score\"]\n",
    "        print (\"Set\", s, \": Essays = \", n , \"\\t Attributes = \", d)\n",
    "        min_scores.append(set_scores.min())\n",
    "        max_scores.append(set_scores.max())\n",
    "        essay_sets.append(essay_set)\n",
    "    return (essay_sets, min_scores, max_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1 : Essays =  1783 \t Attributes =  6\n",
      "Set 2 : Essays =  1800 \t Attributes =  9\n",
      "Set 3 : Essays =  1726 \t Attributes =  6\n",
      "Set 4 : Essays =  1770 \t Attributes =  6\n",
      "Set 5 : Essays =  1805 \t Attributes =  6\n",
      "Set 6 : Essays =  1800 \t Attributes =  6\n",
      "Set 7 : Essays =  1569 \t Attributes =  14\n",
      "Set 8 : Essays =  723 \t Attributes =  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3722479573.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  essay_set.dropna(axis=1, inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set1.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set2.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set3.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set4.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set5.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set6.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set7.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
      "C:\\Users\\VENKATESH\\AppData\\Local\\Temp\\ipykernel_50396\\3635624410.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  set8.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay   \n",
       "0         1          1  Dear local newspaper, I think effects computer...  \\\n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"D:/NEU/DL/Project/Essay grading/English-Language-Learning-Prediction-with-AI-and-Machine-Learning/ASAP Dataset/training_set_rel3.tsv\"\n",
    "data = pd.read_csv(dataset_path, sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "# data.head()\n",
    "\n",
    "min_scores = [2, 1, 0, 0, 0, 0, 0, 0]\n",
    "max_scores = [12, 6, 3, 3, 4, 4, 30, 60]\n",
    "essay_sets, data_min_scores, data_max_scores = split_in_sets(data)\n",
    "set1, set2, set3, set4, set5, set6, set7, set8 = tuple(essay_sets)\n",
    "data.dropna(axis=1, inplace=True)\n",
    "data.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set1.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set2.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set3.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set4.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set5.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set6.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set7.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "set8.drop(columns=[\"rater1_domain1\", \"rater2_domain1\"], inplace=True)\n",
    "sets = [set1,set2,set3,set4,set5,set6,set7,set8]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, LSTM, Dense, Dropout, Lambda, Flatten, Bidirectional, Conv2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential,Model, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model(Hidden_dim1=400, Hidden_dim2=128, return_sequences = True, dropout=0.5, recurrent_dropout=0.4, input_size=768, activation='relu', bidirectional = False):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    if bidirectional:\n",
    "        model.add(Bidirectional(LSTM(Hidden_dim1,return_sequences=return_sequences , dropout=0.4, recurrent_dropout=recurrent_dropout), input_shape=[1, input_size]))\n",
    "        model.add(Bidirectional(LSTM(Hidden_dim2, recurrent_dropout=recurrent_dropout)))\n",
    "    else:\n",
    "        model.add(LSTM(Hidden_dim1, dropout=0.4, recurrent_dropout=recurrent_dropout, input_shape=[1, input_size], return_sequences=return_sequences))\n",
    "        model.add(LSTM(Hidden_dim2, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_model_CNN(Hidden_dim1=400, Hidden_dim2=128, return_sequences = True, dropout=0.5, recurrent_dropout=0.4, input_size=768,output_dims=10380, activation='relu', bidirectional = False):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    inputs = Input(shape=(768,1))\n",
    "    x = Conv1D(64, 3, strides=1, padding='same', activation='relu')(inputs)\n",
    "    #Cuts the size of the output in half, maxing over every 2 inputs\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(128, 3, strides=1, padding='same', activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x) \n",
    "    outputs = Dense(output_dims, activation='relu')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='CNN')\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae','mse'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Sets experiment BERT\n",
    "import time\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "set_count = 1\n",
    "all_sets_score = []\n",
    "\n",
    "for s in sets:\n",
    "  print(\"\\n--------SET {}--------\\n\".format(set_count))\n",
    "  X = s\n",
    "  y = s['domain1_score']\n",
    "  cv = KFold(n_splits=5, shuffle=True)\n",
    "  cv_data = cv.split(X)\n",
    "  results = []\n",
    "  prediction_list = []\n",
    "  fold_count =1\n",
    "  cuda = torch.device('cuda')\n",
    "  # For DistilBERT:\n",
    "  model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "  ## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "  ##model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "  # Load pretrained model/tokenizer\n",
    "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "  model = model_class.from_pretrained(pretrained_weights)\n",
    "  with torch.cuda.device(cuda):\n",
    "    for traincv, testcv in cv_data:\n",
    "      torch.cuda.empty_cache()\n",
    "      print(\"\\n--------Fold {}--------\\n\".format(fold_count))\n",
    "      # get the train and test from the dataset.\n",
    "      X_train, X_test, y_train, y_test = X.iloc[traincv], X.iloc[testcv], y.iloc[traincv], y.iloc[testcv]\n",
    "      train_essays = X_train['essay']\n",
    "      #print(\"y_train\",y_train)\n",
    "      test_essays = X_test['essay']\n",
    "      # model = model.cuda()\n",
    "      #y_train = torch.tensor(y_train,dtype=torch.long)\n",
    "      sentences = []\n",
    "      tokenize_sentences = []\n",
    "      train_bert_embeddings = []\n",
    "      #bert_embedding = BertEmbedding()\n",
    "      # for essay in train_essays:\n",
    "      #   # get all the sentences from the essay\n",
    "      #   sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "      # sentences = pd.Series(sentences)\n",
    "      # print(train_essays)\n",
    "      tokenized_train = train_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "      tokenized_test = test_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "\n",
    "\n",
    "      ## train\n",
    "      max_len = 0\n",
    "      for i in tokenized_train.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "\n",
    "      attention_mask_train = np.where(padded_train != 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "      train_input_ids = torch.tensor(padded_train)\n",
    "      train_attention_mask = torch.tensor(attention_mask_train)\n",
    "      with torch.no_grad():\n",
    "        last_hidden_states_train = model(train_input_ids, attention_mask=train_attention_mask)\n",
    "\n",
    "\n",
    "      train_features = last_hidden_states_train[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "      ## test\n",
    "      max_len = 0\n",
    "      for i in tokenized_test.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "      attention_mask_test = np.where(padded_test != 0, 1, 0)\n",
    "      test_input_ids = torch.tensor(padded_test)  \n",
    "      test_attention_mask = torch.tensor(attention_mask_test)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        last_hidden_states_test = model(test_input_ids, attention_mask=test_attention_mask)\n",
    "\n",
    "      test_features = last_hidden_states_test[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      train_x,train_y = train_features.shape\n",
    "      test_x,test_y = test_features.shape\n",
    "\n",
    "      trainDataVectors = np.reshape(train_features,(train_x,1,train_y))\n",
    "      testDataVectors = np.reshape(test_features,(test_x,1,test_y))\n",
    "\n",
    "      lstm_model = get_model(bidirectional=False)\n",
    "      lstm_model.fit(trainDataVectors, y_train, batch_size=128, epochs=70)\n",
    "      y_pred = lstm_model.predict(testDataVectors)\n",
    "\n",
    "      y_pred = np.around(y_pred)\n",
    "      #y_pred.dropna()\n",
    "      np.nan_to_num(y_pred)\n",
    "      # evaluate the model\n",
    "      result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "      print(\"Kappa Score: {}\".format(result))\n",
    "      results.append(result)\n",
    "      fold_count +=1\n",
    "      import tensorflow as tf\n",
    "      tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "  all_sets_score.append(results)\n",
    "  print(\"Average kappa score value is : {}\".format(np.mean(np.asarray(results))))\n",
    "  set_count+=1\n",
    "    # print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Sets experiment BERT\n",
    "import time\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "set_count = 1\n",
    "all_sets_score = []\n",
    "\n",
    "for s in sets:\n",
    "  print(\"\\n--------SET {}--------\\n\".format(set_count))\n",
    "  X = s\n",
    "  y = s['domain1_score']\n",
    "  cv = KFold(n_splits=5, shuffle=True)\n",
    "  cv_data = cv.split(X)\n",
    "  results = []\n",
    "  prediction_list = []\n",
    "  fold_count =1\n",
    "  cuda = torch.device('cuda')\n",
    "  # For DistilBERT:\n",
    "  model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "  ## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "  ##model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "  # Load pretrained model/tokenizer\n",
    "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "  model = model_class.from_pretrained(pretrained_weights)\n",
    "  with torch.cuda.device(cuda):\n",
    "    for traincv, testcv in cv_data:\n",
    "      torch.cuda.empty_cache()\n",
    "      print(\"\\n--------Fold {}--------\\n\".format(fold_count))\n",
    "      # get the train and test from the dataset.\n",
    "      X_train, X_test, y_train, y_test = X.iloc[traincv], X.iloc[testcv], y.iloc[traincv], y.iloc[testcv]\n",
    "      train_essays = X_train['essay']\n",
    "      #print(\"y_train\",y_train)\n",
    "      test_essays = X_test['essay']\n",
    "      # model = model.cuda()\n",
    "      #y_train = torch.tensor(y_train,dtype=torch.long)\n",
    "      sentences = []\n",
    "      tokenize_sentences = []\n",
    "      train_bert_embeddings = []\n",
    "      #bert_embedding = BertEmbedding()\n",
    "      # for essay in train_essays:\n",
    "      #   # get all the sentences from the essay\n",
    "      #   sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "      # sentences = pd.Series(sentences)\n",
    "      # print(train_essays)\n",
    "      tokenized_train = train_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "      tokenized_test = test_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "\n",
    "\n",
    "      ## train\n",
    "      max_len = 0\n",
    "      for i in tokenized_train.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "\n",
    "      attention_mask_train = np.where(padded_train != 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "      train_input_ids = torch.tensor(padded_train)\n",
    "      train_attention_mask = torch.tensor(attention_mask_train)\n",
    "      with torch.no_grad():\n",
    "        last_hidden_states_train = model(train_input_ids, attention_mask=train_attention_mask)\n",
    "\n",
    "\n",
    "      train_features = last_hidden_states_train[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "      ## test\n",
    "      max_len = 0\n",
    "      for i in tokenized_test.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "      attention_mask_test = np.where(padded_test != 0, 1, 0)\n",
    "      test_input_ids = torch.tensor(padded_test)  \n",
    "      test_attention_mask = torch.tensor(attention_mask_test)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        last_hidden_states_test = model(test_input_ids, attention_mask=test_attention_mask)\n",
    "\n",
    "      test_features = last_hidden_states_test[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      train_x,train_y = train_features.shape\n",
    "      test_x,test_y = test_features.shape\n",
    "\n",
    "      trainDataVectors = np.reshape(train_features,(train_x,1,train_y))\n",
    "      testDataVectors = np.reshape(test_features,(test_x,1,test_y))\n",
    "\n",
    "      lstm_model = get_model(bidirectional=False)\n",
    "      lstm_model.fit(trainDataVectors, y_train, batch_size=128, epochs=70)\n",
    "      y_pred = lstm_model.predict(testDataVectors)\n",
    "\n",
    "      y_pred = np.around(y_pred)\n",
    "      #y_pred.dropna()\n",
    "      np.nan_to_num(y_pred)\n",
    "      # evaluate the model\n",
    "      result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "      print(\"Kappa Score: {}\".format(result))\n",
    "      results.append(result)\n",
    "      fold_count +=1\n",
    "      import tensorflow as tf\n",
    "      tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "  all_sets_score.append(results)\n",
    "  print(\"Average kappa score value is : {}\".format(np.mean(np.asarray(results))))\n",
    "  set_count+=1\n",
    "    # print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Sets experiment BERT\n",
    "import time\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "set_count = 1\n",
    "all_sets_score = []\n",
    "all_sets_avg=[]\n",
    "for s in sets:\n",
    "  print(\"\\n--------SET {}--------\\n\".format(set_count))\n",
    "  X = s\n",
    "  y = s['domain1_score']\n",
    "  cv = KFold(n_splits=5, shuffle=True)\n",
    "  cv_data = cv.split(X)\n",
    "  results = []\n",
    "  prediction_list = []\n",
    "  fold_count =1\n",
    "  cuda = torch.device('cuda')\n",
    "  # For DistilBERT:\n",
    "  model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "  ## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "  ##model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "  # Load pretrained model/tokenizer\n",
    "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "  model = model_class.from_pretrained(pretrained_weights)\n",
    "  with torch.cuda.device(cuda):\n",
    "    for traincv, testcv in cv_data:\n",
    "      torch.cuda.empty_cache()\n",
    "      print(\"\\n--------Fold {}--------\\n\".format(fold_count))\n",
    "      # get the train and test from the dataset.\n",
    "      X_train, X_test, y_train, y_test = X.iloc[traincv], X.iloc[testcv], y.iloc[traincv], y.iloc[testcv]\n",
    "      train_essays = X_train['preprocessed_text']\n",
    "      #print(\"y_train\",y_train)\n",
    "      test_essays = X_test['preprocessed_text']\n",
    "      # model = model.cuda()\n",
    "      #y_train = torch.tensor(y_train,dtype=torch.long)\n",
    "      sentences = []\n",
    "      tokenize_sentences = []\n",
    "      train_bert_embeddings = []\n",
    "      #bert_embedding = BertEmbedding()\n",
    "      # for essay in train_essays:\n",
    "      #   # get all the sentences from the essay\n",
    "      #   sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "      # sentences = pd.Series(sentences)\n",
    "      # print(train_essays)\n",
    "      tokenized_train = train_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "      tokenized_test = test_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "\n",
    "\n",
    "      ## train\n",
    "      max_len = 0\n",
    "      for i in tokenized_train.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "\n",
    "      attention_mask_train = np.where(padded_train != 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "      train_input_ids = torch.tensor(padded_train)\n",
    "      train_attention_mask = torch.tensor(attention_mask_train)\n",
    "      with torch.no_grad():\n",
    "        last_hidden_states_train = model(train_input_ids, attention_mask=train_attention_mask)\n",
    "\n",
    "\n",
    "      train_features = last_hidden_states_train[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "      ## test\n",
    "      max_len = 0\n",
    "      for i in tokenized_test.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "      attention_mask_test = np.where(padded_test != 0, 1, 0)\n",
    "      test_input_ids = torch.tensor(padded_test)  \n",
    "      test_attention_mask = torch.tensor(attention_mask_test)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        last_hidden_states_test = model(test_input_ids, attention_mask=test_attention_mask)\n",
    "\n",
    "      test_features = last_hidden_states_test[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "      train_x,train_y = train_features.shape\n",
    "      test_x,test_y = test_features.shape\n",
    "\n",
    "      trainDataVectors = np.reshape(train_features,(train_x,1,train_y))\n",
    "      testDataVectors = np.reshape(test_features,(test_x,1,test_y))\n",
    "\n",
    "      lstm_model = get_model(bidirectional=False)\n",
    "      lstm_model.fit(trainDataVectors, y_train, batch_size=128, epochs=70)\n",
    "      y_pred = lstm_model.predict(testDataVectors)\n",
    "\n",
    "      y_pred = np.around(y_pred)\n",
    "      #y_pred.dropna()\n",
    "      np.nan_to_num(y_pred)\n",
    "      # evaluate the model\n",
    "      result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "      print(\"Kappa Score: {}\".format(result))\n",
    "      results.append(result)\n",
    "      fold_count +=1\n",
    "      \n",
    "      tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "  all_sets_score.append(results)\n",
    "  avg=np.mean(np.asarray(results))\n",
    "  print(\"Average kappa score value is : {}\".format(avg))\n",
    "  all_sets_avg.append(avg)\n",
    "  set_count+=1\n",
    "    # print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XLNET\n",
    "\n",
    "## Sets experiment BERT\n",
    "import time\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "set_count = 1\n",
    "all_sets_score = []\n",
    "all_sets_avg=[]\n",
    "\n",
    "for s in sets:\n",
    "  print(\"\\n--------SET {}--------\\n\".format(set_count))\n",
    "  X = s\n",
    "  y = s['domain1_score']\n",
    "  cv = KFold(n_splits=5, shuffle=True)\n",
    "  cv_data = cv.split(X)\n",
    "  results = []\n",
    "  prediction_list = []\n",
    "  fold_count =1\n",
    "  cuda = torch.device('cuda')\n",
    "\n",
    "  # For DistilBERT:\n",
    "  model_class, tokenizer_class, pretrained_weights = (ppb.XLNetModel, ppb.XLNetTokenizer, 'xlnet-base-cased')\n",
    "  \n",
    "  tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "  model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "  with torch.cuda.device(cuda):\n",
    "    for traincv, testcv in cv_data:\n",
    "      torch.cuda.empty_cache()\n",
    "      print(\"\\n--------Fold {}--------\\n\".format(fold_count))\n",
    "\n",
    "      \n",
    "      X_train, X_test, y_train, y_test = X.iloc[traincv], X.iloc[testcv], y.iloc[traincv], y.iloc[testcv]\n",
    "      train_essays = X_train['preprocessed_text']\n",
    "      \n",
    "      test_essays = X_test['preprocessed_text']\n",
    "      \n",
    "      sentences = []\n",
    "      tokenize_sentences = []\n",
    "      train_bert_embeddings = []\n",
    "      tokenized_train = train_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "      tokenized_test = test_essays.apply((lambda x: tokenizer.encode(x, add_special_tokens=True ,max_length=200)))\n",
    "\n",
    "\n",
    "      ## train\n",
    "      max_len = 0\n",
    "      for i in tokenized_train.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "\n",
    "      attention_mask_train = np.where(padded_train != 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "      train_input_ids = torch.tensor(padded_train)\n",
    "      train_attention_mask = torch.tensor(attention_mask_train)\n",
    "      with torch.no_grad():\n",
    "        # last_hidden_states_train = model(train_input_ids, attention_mask=train_attention_mask)\n",
    "        outputs= model(train_input_ids, attention_mask=train_attention_mask)\n",
    "        last_hidden_states_train = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      train_features = last_hidden_states_train[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "      ## test\n",
    "      max_len = 0\n",
    "      for i in tokenized_test.values:\n",
    "        if len(i) > max_len:\n",
    "          max_len = len(i)\n",
    "      padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "      attention_mask_test = np.where(padded_test != 0, 1, 0)\n",
    "      test_input_ids = torch.tensor(padded_test)  \n",
    "      test_attention_mask = torch.tensor(attention_mask_test)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        # last_hidden_states_test = model(test_input_ids, attention_mask=test_attention_mask)\n",
    "        outputs= model(test_input_ids, attention_mask=test_attention_mask)\n",
    "        last_hidden_states_test = outputs.last_hidden_state\n",
    "\n",
    "      test_features = last_hidden_states_test[0][:,0,:].numpy()\n",
    "      \n",
    "\n",
    "      train_x,train_y = train_features.shape\n",
    "      test_x,test_y = test_features.shape\n",
    "\n",
    "      trainDataVectors = np.reshape(train_features,(train_x,1,train_y))\n",
    "      testDataVectors = np.reshape(test_features,(test_x,1,test_y))\n",
    "\n",
    "      lstm_model = get_model(bidirectional=False)\n",
    "      lstm_model.fit(trainDataVectors, y_train, batch_size=128, epochs=10)\n",
    "      y_pred = lstm_model.predict(testDataVectors)\n",
    "\n",
    "      y_pred = np.around(y_pred)\n",
    "      #y_pred.dropna()\n",
    "      np.nan_to_num(y_pred)\n",
    "      # evaluate the model\n",
    "      result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "      print(\"Kappa Score: {}\".format(result))\n",
    "      results.append(result)\n",
    "      fold_count +=1\n",
    "      \n",
    "      tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "  all_sets_score.append(results)\n",
    "  avg=np.mean(np.asarray(results))\n",
    "  print(\"Average kappa score value is : {}\".format(avg))\n",
    "  all_sets_avg.append(avg)\n",
    "  set_count+=1\n",
    "    # print(features.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
